---
phase: 01-foundation
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - src/astraea/io/__init__.py
  - src/astraea/io/sas_reader.py
  - src/astraea/profiling/__init__.py
  - src/astraea/profiling/profiler.py
  - tests/test_io/test_sas_reader.py
  - tests/test_profiling/test_profiler.py
autonomous: true

must_haves:
  truths:
    - "System reads any .sas7bdat file and returns DataFrame + rich metadata"
    - "System profiles a dataset showing variable names, types, labels, row counts, value distributions, missing data patterns, and detected date formats"
    - "System correctly identifies EDC system columns vs clinical data columns"
  artifacts:
    - path: "src/astraea/io/sas_reader.py"
      provides: "SAS file reading with metadata extraction"
      exports: ["read_sas_with_metadata", "read_all_sas_files"]
    - path: "src/astraea/profiling/profiler.py"
      provides: "Dataset profiling logic"
      exports: ["profile_dataset"]
  key_links:
    - from: "src/astraea/io/sas_reader.py"
      to: "src/astraea/models/metadata.py"
      via: "returns DatasetMetadata"
      pattern: "DatasetMetadata"
    - from: "src/astraea/profiling/profiler.py"
      to: "src/astraea/models/profiling.py"
      via: "returns DatasetProfile"
      pattern: "DatasetProfile"
    - from: "src/astraea/profiling/profiler.py"
      to: "src/astraea/io/sas_reader.py"
      via: "uses metadata from reader"
      pattern: "DatasetMetadata"
---

<objective>
Build the SAS file reader and dataset profiler -- the data ingestion layer that every downstream component depends on.

Purpose: Satisfies DATA-01 (read SAS files, extract metadata) and DATA-02 (profile datasets automatically). This is the first thing users interact with.
Output: Working SAS reader and profiler with tests verified against Fakedata/ sample files.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation/01-RESEARCH.md
@.planning/phases/01-foundation/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement SAS reader with metadata extraction</name>
  <files>
    src/astraea/io/sas_reader.py
    src/astraea/io/__init__.py
    tests/test_io/test_sas_reader.py
  </files>
  <action>
    Implement SAS file reading using pyreadstat.

    **read_sas_with_metadata(filepath: str | Path) -> tuple[pd.DataFrame, DatasetMetadata]:**
    - Call pyreadstat.read_sas7bdat with disable_datetime_conversion=True (critical -- keeps raw numeric dates)
    - Extract variable metadata from pyreadstat meta object:
      - meta.column_names -> variable names
      - meta.column_names_to_labels -> labels
      - meta.original_variable_types -> SAS formats (use to determine dtype: "$" = character, else numeric)
      - meta.number_rows, meta.number_columns -> counts
      - meta.file_encoding -> encoding
    - Return (DataFrame, DatasetMetadata) tuple

    **read_all_sas_files(data_dir: str | Path) -> dict[str, tuple[pd.DataFrame, DatasetMetadata]]:**
    - Glob for *.sas7bdat files in directory
    - Call read_sas_with_metadata for each
    - Return dict keyed by filename stem (e.g., "dm", "ae")
    - Log each file read with loguru

    **Tests (test_sas_reader.py):**
    - Test read_sas_with_metadata against Fakedata/dm.sas7bdat:
      - Assert returns tuple of (DataFrame, DatasetMetadata)
      - Assert row_count > 0
      - Assert variables list is non-empty
      - Assert file_encoding is populated
    - Test read_all_sas_files against Fakedata/ directory:
      - Assert returns dict with multiple keys
      - Assert "dm" key exists
    - Test with non-existent file raises appropriate error
  </action>
  <verify>Run: pytest tests/test_io/test_sas_reader.py -v</verify>
  <done>SAS reader reads all 36 Fakedata files successfully. Metadata extraction returns correct variable names, labels, types, and encoding.</done>
</task>

<task type="auto">
  <name>Task 2: Implement dataset profiler with EDC column detection</name>
  <files>
    src/astraea/profiling/profiler.py
    src/astraea/profiling/__init__.py
    tests/test_profiling/test_profiler.py
  </files>
  <action>
    Implement dataset profiling that produces rich statistical summaries.

    **profile_dataset(df: pd.DataFrame, meta: DatasetMetadata) -> DatasetProfile:**
    - For each variable, compute: n_total, n_missing, n_unique, missing_pct, sample_values (first 10 unique non-null), top_values (top 5 value counts for variables with <= 100 unique values)
    - Detect EDC system columns using a known-patterns set (lowercase comparison): projectid, project, studyid, environmentname, subjectid, studysiteid, siteid, instanceid, instancename, instancerepeatnumber, folderid, folder, foldername, folderseq, targetdays, datapageid, datapagename, pagerepeatnumber, recorddate, recordid, recordposition, mincreated, maxupdated, savets, studyenvsitenumber. Mark is_edc_column=True on matching VariableProfile entries.
    - Detect date variables from SAS format: check if sas_format is in ("DATETIME", "DATE", "TIME", "DDMMYY", "MMDDYY", "YYMMDD", "DTDATE", "DATETIME22.3", "DATETIME20.")
      - Also detect string dates: if column name contains "_RAW" and "DAT" (case insensitive), sample non-null values and look for patterns like "DD Mon YYYY", "DD/MM/YYYY", etc.
    - detect_date_format(samples: list[str]) -> str | None: helper that examines sample string values and returns the likely format string (e.g., "DD Mon YYYY") or None

    **Tests (test_profiler.py):**
    - Read Fakedata/dm.sas7bdat with sas_reader, then profile it
    - Assert profile.edc_columns contains known EDC columns (e.g., any of: projectid, instanceId, DataPageId -- check case-insensitively)
    - Assert profile.date_variables is non-empty (DM has DATETIME columns)
    - Assert each variable has n_total == profile.row_count
    - Assert missing_pct is between 0 and 100 for all variables
    - Test profile on a dataset with date columns (e.g., Fakedata/ae.sas7bdat) and verify date detection
  </action>
  <verify>Run: pytest tests/test_profiling/test_profiler.py -v</verify>
  <done>Profiler produces complete DatasetProfile for any SAS file. EDC columns correctly identified. Date variables detected from both SAS format metadata and string pattern analysis.</done>
</task>

</tasks>

<verification>
- `pytest tests/test_io/ tests/test_profiling/ -v` all pass
- `python -c "from astraea.io import read_sas_with_metadata; df, meta = read_sas_with_metadata('Fakedata/dm.sas7bdat'); print(f'{meta.row_count} rows, {meta.col_count} cols')"` prints correct counts
- `python -c "from astraea.io import read_sas_with_metadata; from astraea.profiling import profile_dataset; df, meta = read_sas_with_metadata('Fakedata/dm.sas7bdat'); p = profile_dataset(df, meta); print(f'EDC cols: {p.edc_columns}'); print(f'Date vars: {p.date_variables}')"` shows detected EDC columns and date variables
</verification>

<success_criteria>
- read_sas_with_metadata reads any .sas7bdat file returning DataFrame + DatasetMetadata
- read_all_sas_files reads entire directory of SAS files
- profile_dataset returns DatasetProfile with complete statistics, EDC column detection, and date format detection
- Tests pass against real Fakedata/ sample files
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-02-SUMMARY.md`
</output>
