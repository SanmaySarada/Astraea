---
phase: 07-validation-submission-readiness
plan: 06
type: execute
wave: 3
depends_on: ["07-03"]
files_modified:
  - src/astraea/submission/csdrg.py
  - src/astraea/submission/package.py
  - src/astraea/validation/report.py
  - src/astraea/validation/known_false_positives.json
  - tests/unit/submission/test_csdrg.py
  - tests/unit/submission/test_package.py
autonomous: true

must_haves:
  truths:
    - "System generates a cSDRG template document with domain mapping rationale and validation summary"
    - "System produces a validation report with severity categorization and submission readiness score"
    - "Dataset size validation checks total submission against 5GB FDA limit"
    - "File naming conventions enforced (lowercase domain codes, .xpt extension)"
    - "Known false positives are flagged in validation report output, not counted as real errors"
    - "Oversized XPT files get domain-specific split guidance (e.g., split LB by LBCAT)"
  artifacts:
    - path: "src/astraea/submission/csdrg.py"
      provides: "cSDRG template generator using Jinja2"
      contains: "def generate_csdrg"
    - path: "src/astraea/submission/package.py"
      provides: "Submission package assembly and size check"
      contains: "def check_submission_size"
    - path: "src/astraea/validation/known_false_positives.json"
      provides: "Config of known false-positive rule+domain+variable tuples"
      contains: "rule_id"
  key_links:
    - from: "src/astraea/submission/csdrg.py"
      to: "src/astraea/models/mapping.py"
      via: "reads DomainMappingSpec for domain sections"
      pattern: "DomainMappingSpec"
    - from: "src/astraea/submission/package.py"
      to: "src/astraea/validation/rules/fda_trc.py"
      via: "uses TRCPreCheck for submission validation"
      pattern: "TRCPreCheck"
    - from: "src/astraea/validation/report.py"
      to: "src/astraea/validation/known_false_positives.json"
      via: "loads whitelist to flag matching RuleResults"
      pattern: "known_false_positives"
---

<objective>
Build the cSDRG (Clinical Study Data Reviewer's Guide) template generator, submission package assembly with size validation, known false-positive whitelist, and enhance the ValidationReport for pre-submission reporting.

Purpose: Complete the submission artifact set (cSDRG + validation report + package checks) required for FDA submission readiness.
Output: submission/csdrg.py, submission/package.py, enhanced report.py, known_false_positives.json
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/07-validation-submission-readiness/07-RESEARCH.md
@src/astraea/validation/rules/base.py
@src/astraea/validation/report.py
@src/astraea/validation/rules/fda_trc.py
@src/astraea/models/mapping.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: cSDRG template generator with known false-positive whitelist</name>
  <files>
    src/astraea/submission/csdrg.py
    src/astraea/validation/known_false_positives.json
    src/astraea/validation/report.py
    tests/unit/submission/test_csdrg.py
  </files>
  <action>
First, add `jinja2` to pyproject.toml dependencies if not already present.

**known_false_positives.json** -- Known false-positive whitelist config:

Create `src/astraea/validation/known_false_positives.json` with this structure:
```json
{
  "description": "Known false positives from P21/CORE validation. Matching RuleResults are flagged as known_false_positive=true in report output.",
  "version": "1.0",
  "entries": [
    {
      "rule_id": "SD1076",
      "domain": "LB",
      "variable": "LBSTRESC",
      "reason": "P21 v2405.2 known issue: false positive for numeric LBSTRESC values"
    }
  ]
}
```

Each entry has: rule_id (str, required), domain (str|null -- null means any domain), variable (str|null -- null means any variable), reason (str). A RuleResult matches if rule_id matches AND (domain is null OR domain matches) AND (variable is null OR variable matches).

**Enhance report.py** -- Add known false-positive support to ValidationReport:

1. Add `is_known_false_positive: bool = False` field to RuleResult in base.py (or if base.py is frozen, add a post-processing method to ValidationReport).
   - Preferred approach: Add a method `flag_known_false_positives(self, whitelist_path: Path | None = None) -> None` to ValidationReport that:
     - Loads known_false_positives.json from the default path (`Path(__file__).parent / "known_false_positives.json"`) or whitelist_path
     - Iterates self.results and sets a `known_false_positive` tag on matching RuleResults
     - Implementation: Add `known_false_positive: bool = False` to RuleResult model in base.py
   - Recalculates error_count/warning_count excluding known false positives (add `effective_error_count` and `effective_warning_count` properties that exclude flagged results)
   - Updates submission_ready to use effective counts

2. Update `from_results()` classmethod to accept optional `whitelist_path` parameter and call `flag_known_false_positives()` after construction.

**csdrg.py** -- cSDRG Markdown document generator:

`generate_csdrg(specs: list[DomainMappingSpec], validation_report: ValidationReport, study_id: str, output_path: Path, *, sdtm_ig_version: str = "3.4", ct_version: str | None = None) -> Path`

Use Jinja2 to render a Markdown template inline (string template, not file-based). The template follows PHUSE cSDRG structure:

**Section 1 - Introduction**: Study ID, purpose of guide, generated timestamp.

**Section 2 - Study Description**: "[Placeholder: Add study description, trial design, objectives, endpoints]" -- user fills this in.

**Section 3 - Data Standards and Dictionary Inventory**: Table with SDTM-IG version, CT version (from params), MedDRA version (placeholder), data standard (SDTM).

**Section 4 - Dataset Overview**: Table listing all domains with: Domain code, Domain label, Domain class, Number of records (from spec.total_variables), Structure, Source datasets.

**Section 5 - Domain-Specific Information**: For each domain spec:
- Source data description (list source_datasets)
- Mapping approach summary (count of each mapping pattern used)
- Non-standard variables (any mappings where variable not in SDTM-IG)
- SUPPQUAL candidates (from spec.suppqual_candidates)
- Missing required variables (from spec.missing_required_variables)

**Section 6 - Data Issues and Handling**: Date imputation rules (partial dates -> ISO 8601 truncation, per SDTM-IG convention). Missing data conventions. Known data issues (placeholder for user).

**Section 7 - Validation Results Summary**: Error count, warning count, notice count from validation_report. Use effective_error_count/effective_warning_count if available. Top 5 most common issues. **Known False Positives subsection**: List any RuleResults flagged as known_false_positive with their rule_id, domain, variable, and reason from the whitelist. Format as a table so reviewers can see exactly which findings were excluded and why.

**Section 8 - Non-Standard Variables**: Table of all SUPPQUAL candidates across all domains with justification (from mapping notes).

Write rendered Markdown to output_path.

**Tests**:
1. Generate cSDRG with 2 synthetic specs + synthetic ValidationReport
2. Verify output file exists and is valid Markdown
3. Verify all 8 sections present (check for section headers)
4. Verify domain overview table has correct domain count
5. Verify validation summary includes error/warning counts
6. Test known false-positive flagging: create RuleResult matching a whitelist entry, call flag_known_false_positives(), verify is_known_false_positive=True and effective counts exclude it
7. Verify cSDRG Section 7 includes "Known False Positives" subsection when flagged results exist
  </action>
  <verify>pytest tests/unit/submission/test_csdrg.py -x -q</verify>
  <done>cSDRG template generates 8-section Markdown document; known false positives flagged in report and listed in cSDRG Section 7</done>
</task>

<task type="auto">
  <name>Task 2: Submission package assembly and size check with domain-specific split guidance</name>
  <files>
    src/astraea/submission/package.py
    src/astraea/validation/report.py
    tests/unit/submission/test_package.py
  </files>
  <action>
**package.py** -- Submission package utilities:

`check_submission_size(output_dir: Path, *, limit_gb: float = 5.0) -> list[RuleResult]`:
- Walk output_dir, sum sizes of all .xpt files
- If total > limit_gb * 1024^3: return ERROR result with split recommendation
- If any single file > 1GB: return WARNING with **domain-specific split guidance** in fix_suggestion. Use a lookup dict for common large domains:
  ```python
  SPLIT_GUIDANCE = {
      "lb": "Split LB by LBCAT into separate XPT files (e.g., lb_chem.xpt, lb_hem.xpt, lb_ua.xpt)",
      "ae": "Split AE by AESEV or AESER into separate XPT files if needed",
      "cm": "Split CM by CMCAT into separate XPT files (e.g., cm_prior.xpt, cm_concom.xpt)",
      "eg": "Split EG by EGTESTCD into separate XPT files (e.g., eg_rhythm.xpt, eg_interval.xpt)",
      "vs": "Split VS by VSTESTCD grouping into separate XPT files",
      "fa": "Split FA by FATESTCD or parent domain into separate XPT files",
  }
  ```
  Extract domain code from filename (e.g., "lb.xpt" -> "lb"). Look up in SPLIT_GUIDANCE. If found, include in fix_suggestion. If not found, use generic guidance: "Consider splitting {filename} by a categorical variable to reduce file size below 1GB."
- Return NOTICE with total size and per-file breakdown regardless

`validate_file_naming(output_dir: Path, expected_domains: list[str]) -> list[RuleResult]`:
- For each expected domain: check {domain.lower()}.xpt exists
- Check no unexpected .xpt files (warning for extras)
- Check define.xml exists (error if missing)
- Severity: ERROR for missing expected files, WARNING for extra files

`assemble_package_manifest(output_dir: Path, specs: list[DomainMappingSpec]) -> dict`:
- Return dict with: file inventory (path, size, domain), total_size, domain_count, has_define_xml, has_csdrg
- This is informational metadata for the user

**Enhance report.py** -- Add to ValidationReport:
- `to_markdown(self) -> str` method: Renders the validation report as a Markdown document with:
  - Header with study_id and generated_at
  - Summary table: domains validated, total errors/warnings/notices, submission_ready flag
  - Use effective_error_count/effective_warning_count (excluding known false positives) for the summary if available
  - Per-domain breakdown table
  - Per-category breakdown table
  - Top 10 issues (sorted by severity then affected_count)
  - Known false positives section (if any flagged) with rule_id, domain, variable, reason
  - Submission readiness assessment (READY / NOT READY with blocking issues listed)

**Tests**:
1. check_submission_size with small dir (under limit) returns NOTICE
2. check_submission_size with mocked large files returns ERROR
3. **check_submission_size with mocked lb.xpt > 1GB returns WARNING with fix_suggestion containing "Split LB by LBCAT"**
4. **check_submission_size with mocked unknown_domain.xpt > 1GB returns WARNING with generic split guidance**
5. validate_file_naming with correct files returns empty errors
6. validate_file_naming with missing file returns ERROR
7. assemble_package_manifest returns correct file inventory
8. ValidationReport.to_markdown() produces valid Markdown with all sections
  </action>
  <verify>pytest tests/unit/submission/test_package.py -x -q</verify>
  <done>Package size check with domain-specific split guidance, file naming validation, manifest assembly, and report Markdown export all work</done>
</task>

</tasks>

<verification>
- `pytest tests/unit/submission/ tests/unit/validation/test_engine.py -x -q` -- all tests pass
- `ruff check src/astraea/submission/ src/astraea/validation/report.py`
- cSDRG generates 8-section document with known false positives in Section 7
- Package size check enforces 5GB limit with domain-specific split guidance for >1GB files
- Known false-positive whitelist loaded and applied to validation results
</verification>

<success_criteria>
- cSDRG template generates complete 8-section Markdown with domain details and validation summary
- Known false positives flagged via JSON config, excluded from effective error counts, listed in cSDRG
- Submission package size checked against 5GB FDA limit
- Files > 1GB get domain-specific split guidance (LB by LBCAT, etc.)
- File naming conventions validated (lowercase domain.xpt)
- ValidationReport.to_markdown() produces readable pre-submission report
- 12+ unit tests covering cSDRG, package, report, and false-positive flagging
</success_criteria>

<output>
After completion, create `.planning/phases/07-validation-submission-readiness/07-06-SUMMARY.md`
</output>
