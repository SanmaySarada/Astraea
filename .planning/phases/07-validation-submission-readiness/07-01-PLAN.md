---
phase: 07-validation-submission-readiness
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/astraea/validation/__init__.py
  - src/astraea/validation/rules/__init__.py
  - src/astraea/validation/rules/base.py
  - src/astraea/validation/engine.py
  - src/astraea/validation/report.py
  - tests/unit/validation/test_base_rules.py
  - tests/unit/validation/test_engine.py
autonomous: true

must_haves:
  truths:
    - "Validation rules can be instantiated with Pydantic models and evaluated against DataFrames"
    - "Validation engine discovers and runs rules by category, returning structured results"
    - "Validation report aggregates results with severity counts, pass rates, and domain breakdown"
  artifacts:
    - path: "src/astraea/validation/rules/base.py"
      provides: "RuleSeverity, RuleCategory, RuleResult, ValidationRule base models"
      contains: "class ValidationRule"
    - path: "src/astraea/validation/engine.py"
      provides: "ValidationEngine orchestrator with rule registry"
      contains: "class ValidationEngine"
    - path: "src/astraea/validation/report.py"
      provides: "ValidationReport model with severity counts and submission readiness score"
      contains: "class ValidationReport"
  key_links:
    - from: "src/astraea/validation/engine.py"
      to: "src/astraea/validation/rules/base.py"
      via: "imports ValidationRule, RuleResult"
      pattern: "from astraea\\.validation\\.rules\\.base import"
    - from: "src/astraea/validation/report.py"
      to: "src/astraea/validation/rules/base.py"
      via: "imports RuleResult, RuleSeverity, RuleCategory"
      pattern: "from astraea\\.validation\\.rules\\.base import"
---

<objective>
Create the validation framework: Pydantic models for rules, results, and reports, plus the ValidationEngine orchestrator that discovers/runs rules and aggregates results.

Purpose: Foundation for all validation rules (VAL-01 through VAL-07). Every subsequent plan registers rules into this engine.
Output: validation/rules/base.py (models), validation/engine.py (orchestrator), validation/report.py (report model)
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-validation-submission-readiness/07-RESEARCH.md
@src/astraea/models/mapping.py
@src/astraea/models/sdtm.py
@src/astraea/reference/sdtm_ig.py
@src/astraea/reference/controlled_terms.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Validation rule base models and engine</name>
  <files>
    src/astraea/validation/__init__.py
    src/astraea/validation/rules/__init__.py
    src/astraea/validation/rules/base.py
    src/astraea/validation/engine.py
  </files>
  <action>
Create the validation framework:

**base.py** - Pydantic models:
- `RuleSeverity(StrEnum)`: ERROR, WARNING, NOTICE (not Error/Warning/Notice -- use uppercase for consistency with StrEnum convention, but display_name property for display)
- `RuleCategory(StrEnum)`: TERMINOLOGY (VAL-01), PRESENCE (VAL-02), CONSISTENCY (VAL-03), LIMIT (VAL-04), FORMAT (VAL-05), FDA_BUSINESS, FDA_TRC
- `RuleResult(BaseModel)`: rule_id (str), rule_description (str), category (RuleCategory), severity (RuleSeverity), domain (str|None), variable (str|None), message (str), affected_count (int=0), fix_suggestion (str|None), p21_equivalent (str|None)
- `ValidationRule(BaseModel)`: rule_id (str), description (str), category (RuleCategory), severity (RuleSeverity), with abstract `evaluate()` method that takes domain (str), df (pd.DataFrame), spec (DomainMappingSpec), sdtm_ref (SDTMReference), ct_ref (CTReference) and returns list[RuleResult]. Use model_config = ConfigDict(arbitrary_types_allowed=True) since it receives DataFrames.

**engine.py** - ValidationEngine class:
- `__init__(self, *, sdtm_ref: SDTMReference, ct_ref: CTReference)` -- stores references
- `_rules: list[ValidationRule]` -- registered rules
- `register(self, rule: ValidationRule)` -- adds a rule to the registry
- `register_defaults(self)` -- imports and registers all built-in rules from rules/ submodules (call this in __init__)
- `validate_domain(self, domain: str, df: pd.DataFrame, spec: DomainMappingSpec) -> list[RuleResult]` -- runs all registered rules against one domain, returns all results
- `validate_all(self, domains: dict[str, tuple[pd.DataFrame, DomainMappingSpec]]) -> list[RuleResult]` -- runs validate_domain for each domain
- `filter_results(self, results: list[RuleResult], *, category: RuleCategory|None=None, severity: RuleSeverity|None=None, domain: str|None=None) -> list[RuleResult]` -- utility filter

For register_defaults: use a try/except import pattern so the engine works even when rule modules are empty (plans 02-03 fill them in). Initially just register nothing.

**__init__.py** files: export key classes.
  </action>
  <verify>
    python -c "from astraea.validation.rules.base import RuleSeverity, RuleCategory, RuleResult, ValidationRule; from astraea.validation.engine import ValidationEngine; print('OK')"
  </verify>
  <done>ValidationRule base class, RuleResult model, ValidationEngine orchestrator all importable and functional</done>
</task>

<task type="auto">
  <name>Task 2: ValidationReport model and unit tests</name>
  <files>
    src/astraea/validation/report.py
    tests/unit/validation/__init__.py
    tests/unit/validation/test_base_rules.py
    tests/unit/validation/test_engine.py
  </files>
  <action>
**report.py** - ValidationReport model:
- `ValidationReport(BaseModel)`: study_id (str), domains_validated (list[str]), results (list[RuleResult]), total_rules_run (int), error_count (int, computed from results), warning_count (int), notice_count (int), pass_rate (float -- % of domains with zero errors), submission_ready (bool -- True if error_count==0), generated_at (str -- ISO timestamp), summary_by_domain (dict[str, dict] -- domain -> {errors, warnings, notices}), summary_by_category (dict[str, dict] -- category -> {errors, warnings, notices})
- Add `@classmethod from_results(cls, study_id: str, results: list[RuleResult], domains: list[str]) -> ValidationReport` that computes all summary fields from raw results.

**test_base_rules.py** -- test:
- RuleSeverity/RuleCategory enum values
- RuleResult creation with all fields
- RuleResult creation with minimal fields (defaults)
- ValidationRule subclass can be created and evaluate() called
- Create a simple concrete TestRule that always returns one result, verify it works

**test_engine.py** -- test:
- ValidationEngine can be created with mock SDTMReference and CTReference
- register() adds rules
- validate_domain() runs registered rules and returns results
- validate_all() runs across multiple domains
- filter_results() filters by category, severity, domain
- Engine with no rules returns empty results
  </action>
  <verify>pytest tests/unit/validation/ -x -q</verify>
  <done>ValidationReport model computes summaries correctly; all engine and base model tests pass</done>
</task>

</tasks>

<verification>
- `python -c "from astraea.validation import ValidationEngine"` succeeds
- `pytest tests/unit/validation/ -x -q` -- all tests pass
- `ruff check src/astraea/validation/ tests/unit/validation/`
</verification>

<success_criteria>
- ValidationRule base class exists with evaluate() signature matching the research pattern
- ValidationEngine orchestrates rule execution with register/validate_domain/validate_all/filter_results
- ValidationReport aggregates results with severity counts and submission_ready flag
- 12+ unit tests covering models, engine, and report
</success_criteria>

<output>
After completion, create `.planning/phases/07-validation-submission-readiness/07-01-SUMMARY.md`
</output>
