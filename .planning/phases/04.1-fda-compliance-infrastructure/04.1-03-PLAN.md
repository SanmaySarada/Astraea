---
phase: 04.1-fda-compliance-infrastructure
plan: 03
type: execute
wave: 2
depends_on: ["04.1-01", "04.1-02"]
files_modified:
  - src/astraea/execution/__init__.py
  - src/astraea/execution/pattern_handlers.py
  - src/astraea/execution/executor.py
  - tests/unit/execution/__init__.py
  - tests/unit/execution/test_pattern_handlers.py
  - tests/unit/execution/test_executor.py
autonomous: true

must_haves:
  truths:
    - "Each MappingPattern enum value has a corresponding handler function"
    - "DatasetExecutor transforms DomainMappingSpec + raw DataFrame into SDTM DataFrame"
    - "ASSIGN pattern sets constant values for all rows"
    - "DIRECT/RENAME patterns copy/rename source columns"
    - "LOOKUP_RECODE pattern maps values via codelist"
    - "DERIVATION pattern delegates to transform registry"
    - "Execution pipeline applies patterns in correct order: ASSIGN -> DIRECT/RENAME -> REFORMAT -> LOOKUP_RECODE -> DERIVATION -> COMBINE"
  artifacts:
    - path: "src/astraea/execution/pattern_handlers.py"
      provides: "Per-pattern handler functions"
      exports: ["PATTERN_HANDLERS", "handle_assign", "handle_direct", "handle_rename", "handle_reformat", "handle_lookup_recode", "handle_derivation"]
    - path: "src/astraea/execution/executor.py"
      provides: "DatasetExecutor class"
      exports: ["DatasetExecutor", "CrossDomainContext"]
  key_links:
    - from: "src/astraea/execution/executor.py"
      to: "src/astraea/execution/pattern_handlers.py"
      via: "PATTERN_HANDLERS dispatch"
      pattern: "PATTERN_HANDLERS\\[.*mapping_pattern"
    - from: "src/astraea/execution/executor.py"
      to: "src/astraea/transforms/"
      via: "study_day, sequence, epoch derivation calls"
      pattern: "calculate_study_day|generate_seq|assign_epoch"
    - from: "src/astraea/execution/executor.py"
      to: "src/astraea/mapping/transform_registry.py"
      via: "get_transform for REFORMAT/DERIVATION patterns"
      pattern: "get_transform"
---

<objective>
Build the dataset execution pipeline that transforms approved DomainMappingSpec + raw DataFrames into SDTM-compliant DataFrames.

Purpose: This is the "execution bridge" -- the most critical gap. Currently the system produces mapping specifications but cannot execute them into actual SDTM datasets. Every domain in Phase 5+ depends on this pipeline.

Output: New src/astraea/execution/ module with pattern handlers and DatasetExecutor class.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04.1-fda-compliance-infrastructure/04.1-RESEARCH.md

@src/astraea/models/mapping.py
@src/astraea/mapping/transform_registry.py
@src/astraea/transforms/__init__.py
@src/astraea/reference/sdtm_ig.py
@src/astraea/reference/controlled_terms.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Pattern handler functions</name>
  <files>
    src/astraea/execution/__init__.py
    src/astraea/execution/pattern_handlers.py
    tests/unit/execution/__init__.py
    tests/unit/execution/test_pattern_handlers.py
  </files>
  <action>
    Create `src/astraea/execution/__init__.py` with re-exports of DatasetExecutor and CrossDomainContext (forward-declare for now, will be implemented in Task 2).

    Create `src/astraea/execution/pattern_handlers.py` with individual handler functions and a dispatch registry.

    Each handler has signature: `(df: pd.DataFrame, mapping: VariableMapping, **kwargs) -> pd.Series`
    The kwargs allow passing extra context (codelist data, transform registry) without changing the signature.

    Handlers:

    1. `handle_assign(df, mapping, **kwargs) -> pd.Series`:
       - Return Series filled with mapping.assigned_value for all rows
       - Raise ValueError if assigned_value is None

    2. `handle_direct(df, mapping, **kwargs) -> pd.Series`:
       - Copy df[mapping.source_variable]
       - Raise KeyError if source_variable not in df.columns
       - Raise ValueError if source_variable is None

    3. `handle_rename(df, mapping, **kwargs) -> pd.Series`:
       - Same as handle_direct (copy column, it gets a new name at assignment)

    4. `handle_reformat(df, mapping, **kwargs) -> pd.Series`:
       - Get transform function from mapping.derivation_rule via get_transform()
       - If transform found, apply it element-wise to df[mapping.source_variable] using .map()
       - If not found, log warning and return df[mapping.source_variable].copy() (pass through)
       - Common case: sas_datetime_to_iso for date columns

    5. `handle_lookup_recode(df, mapping, **kwargs) -> pd.Series`:
       - Accept optional `ct_reference` in kwargs (CTReference instance)
       - If mapping.codelist_code and ct_reference provided, get codelist terms
       - Build recode dict from codelist terms (term submission value -> term)
       - Apply .map() with the recode dict, keeping original values for non-matches
       - If no codelist available, return source column as-is with log warning

    6. `handle_derivation(df, mapping, **kwargs) -> pd.Series`:
       - Check mapping.derivation_rule for known transform names (via get_transform)
       - If found, apply transform to source column
       - If derivation_rule contains "USUBJID" or "generate_usubjid", use generate_usubjid_column from transforms
       - For unrecognized derivation rules, return a Series of None with a log warning
       - This is intentionally conservative -- unknown derivations are flagged, not guessed

    7. `handle_combine(df, mapping, **kwargs) -> pd.Series`:
       - Parse mapping.derivation_rule to identify source columns
       - If derivation_rule mentions concatenation (common for USUBJID), delegate to generate_usubjid_column
       - Otherwise return Series of None with warning

    8. `handle_split` and `handle_transpose`:
       - Stub implementations that log warning and return Series of None
       - These are complex domain-specific patterns implemented in Phase 5/6

    Registry:
    ```python
    PATTERN_HANDLERS: dict[MappingPattern, Callable] = {
        MappingPattern.ASSIGN: handle_assign,
        MappingPattern.DIRECT: handle_direct,
        MappingPattern.RENAME: handle_rename,
        MappingPattern.REFORMAT: handle_reformat,
        MappingPattern.LOOKUP_RECODE: handle_lookup_recode,
        MappingPattern.DERIVATION: handle_derivation,
        MappingPattern.COMBINE: handle_combine,
        MappingPattern.SPLIT: handle_split,
        MappingPattern.TRANSPOSE: handle_transpose,
    }
    ```

    Tests (test_pattern_handlers.py):
    - Create a fixture DataFrame with columns: Subject="001", AETERM="Headache", AESTDT=22738.0, SEX="Male"
    - test_handle_assign: mapping with assigned_value="AE" -> all rows "AE"
    - test_handle_assign_no_value_raises: assigned_value=None -> ValueError
    - test_handle_direct: source_variable="AETERM" -> copies column
    - test_handle_direct_missing_col_raises: source_variable="NONEXISTENT" -> KeyError
    - test_handle_rename: same behavior as direct
    - test_handle_reformat_with_transform: derivation_rule="sas_datetime_to_iso", source is SAS datetime numeric -> ISO string
    - test_handle_lookup_recode_with_codelist: codelist mapping Male->M, Female->F
    - test_handle_derivation_unknown_rule: derivation_rule="unknown_rule" -> Series of None with warning
    - test_pattern_registry_complete: every MappingPattern enum value has an entry in PATTERN_HANDLERS
    - Create empty tests/unit/execution/__init__.py
  </action>
  <verify>
    pytest tests/unit/execution/test_pattern_handlers.py -x -q
  </verify>
  <done>
    All 9 MappingPattern values have handler functions; ASSIGN/DIRECT/RENAME/REFORMAT/LOOKUP_RECODE/DERIVATION handlers work correctly; SPLIT/TRANSPOSE are stubbed for Phase 5/6; handler registry is complete.
  </done>
</task>

<task type="auto">
  <name>Task 2: DatasetExecutor class</name>
  <files>
    src/astraea/execution/executor.py
    src/astraea/execution/__init__.py
    tests/unit/execution/test_executor.py
  </files>
  <action>
    Create `src/astraea/execution/executor.py` with:

    1. `CrossDomainContext` Pydantic model:
       ```python
       class CrossDomainContext(BaseModel):
           rfstdtc_lookup: dict[str, str] = Field(default_factory=dict)  # USUBJID -> RFSTDTC
           se_data: Any = Field(default=None)  # pd.DataFrame for EPOCH (use Any to avoid Pydantic DataFrame issues)
           tv_data: Any = Field(default=None)  # pd.DataFrame for VISITNUM
           visit_mapping: dict[str, tuple[float, str]] = Field(default_factory=dict)  # raw visit -> (VISITNUM, VISIT)
           model_config = ConfigDict(arbitrary_types_allowed=True)
       ```

    2. `DatasetExecutor` class:
       ```python
       class DatasetExecutor:
           def __init__(
               self,
               *,
               sdtm_ref: SDTMReference | None = None,
               ct_ref: CTReference | None = None,
           ):
       ```

       Main method: `execute(self, spec: DomainMappingSpec, raw_dfs: dict[str, pd.DataFrame], cross_domain: CrossDomainContext | None = None) -> pd.DataFrame`

       Execution steps (in order):
       a. **Merge raw DataFrames**: If multiple source datasets, concatenate them (pd.concat with ignore_index=True). If single source, use it directly. Always work on a COPY.

       b. **Apply mappings by pattern order**: Process mappings in this priority order:
          - First: ASSIGN patterns (constants like STUDYID, DOMAIN)
          - Second: DIRECT and RENAME patterns (column copies)
          - Third: REFORMAT patterns (date conversions etc.)
          - Fourth: LOOKUP_RECODE patterns (codelist mappings)
          - Fifth: DERIVATION and COMBINE patterns (computed fields)
          For each mapping, call PATTERN_HANDLERS[mapping.mapping_pattern](merged_df, mapping, ct_reference=self.ct_ref)
          Assign the returned Series to result_df[mapping.sdtm_variable]

       c. **Derive --DY**: If cross_domain has rfstdtc_lookup and the domain has --DY variables (find date columns ending in "DTC" and create corresponding "DY" columns):
          - Identify all --DTC columns in result_df
          - For each --DTC column, derive the corresponding --DY using calculate_study_day_column
          - Only if the --DY variable exists in the spec's variable_mappings

       d. **Assign EPOCH**: If cross_domain has se_data:
          - Determine date column based on domain class (from spec.domain_class):
            - "Findings": {domain}DTC
            - "Events"/"Interventions": {domain}STDTC
          - Call assign_epoch if the date column exists in result_df

       e. **Assign VISITNUM/VISIT**: If cross_domain has visit_mapping:
          - Call assign_visit with the visit mapping

       f. **Generate --SEQ**: If domain != "DM" (DM has no SEQ):
          - Get sort keys from SDTMReference domain spec key_variables (or use spec metadata)
          - Call generate_seq(result_df, spec.domain, sort_keys)
          - Assign to result_df[f"{spec.domain}SEQ"]

       g. **Enforce variable column order**: Sort columns by the `order` field from variable_mappings. Variables not in mappings go to end.

       h. **Drop unmapped columns**: Keep only columns that appear in variable_mappings (by sdtm_variable name).

       i. **Sort rows**: Sort by domain key_variables from SDTMReference.

       j. **Return result_df**

       Error handling:
       - Wrap each mapping application in try/except, log errors with loguru, continue with other mappings
       - If a critical mapping fails (STUDYID, DOMAIN, USUBJID), raise ExecutionError
       - Non-critical mapping failures: log warning, set column to None

    3. `ExecutionError(Exception)`: Custom exception for execution failures.

    Update `src/astraea/execution/__init__.py` with actual imports of DatasetExecutor, CrossDomainContext, ExecutionError.

    Tests (test_executor.py):
    Build a minimal DM-domain test scenario:
    - Create a simple DomainMappingSpec for DM with 5 mappings:
      - STUDYID: ASSIGN "TEST-001"
      - DOMAIN: ASSIGN "DM"
      - USUBJID: DERIVATION (generate_usubjid)
      - SUBJID: DIRECT from "Subject"
      - SEX: LOOKUP_RECODE from "Gender" with codelist
    - Create a raw DataFrame with: Subject=["001", "002"], Gender=["Male", "Female"]

    - test_executor_basic_dm: Execute and verify result has STUDYID, DOMAIN, SUBJID, SEX columns
    - test_executor_assign_pattern: STUDYID column is "TEST-001" for all rows
    - test_executor_direct_pattern: SUBJID matches raw Subject column
    - test_executor_variable_order: columns are in order defined by mapping order field
    - test_executor_drops_unmapped: raw columns not in spec are not in output
    - test_executor_seq_not_in_dm: DM domain output does not have DMSEQ
    - test_executor_with_cross_domain_dy: add RFSTDTC lookup, verify --DY derivation for a non-DM domain (create minimal AE spec)
    - test_executor_error_handling: mapping references nonexistent source column, executor logs warning and continues
  </action>
  <verify>
    pytest tests/unit/execution/ -x -q
  </verify>
  <done>
    DatasetExecutor.execute() transforms a DomainMappingSpec + raw DataFrames into SDTM DataFrame; patterns applied in correct order; --DY, --SEQ, EPOCH derivations wired to transforms; variable order enforced; unmapped columns dropped; error handling graceful for non-critical failures.
  </done>
</task>

</tasks>

<verification>
pytest tests/unit/execution/ -x -q
python -c "from astraea.execution import DatasetExecutor, CrossDomainContext, ExecutionError; print('Execution module importable')"
ruff check src/astraea/execution/
</verification>

<success_criteria>
- DatasetExecutor transforms DomainMappingSpec + raw data into SDTM DataFrame
- All 9 MappingPattern enum values dispatch to handler functions
- Execution order: ASSIGN -> DIRECT/RENAME -> REFORMAT -> LOOKUP_RECODE -> DERIVATION/COMBINE
- --DY derived from cross-domain RFSTDTC when available
- --SEQ generated for all non-DM domains
- EPOCH assigned from SE data when available
- Variable column order matches SDTM-IG spec order
- Unmapped columns excluded from output
- Non-critical failures logged, not fatal
- All existing 764+ tests still pass
</success_criteria>

<output>
After completion, create `.planning/phases/04.1-fda-compliance-infrastructure/04.1-03-SUMMARY.md`
</output>
