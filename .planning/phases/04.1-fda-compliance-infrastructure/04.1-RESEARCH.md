# Phase 4.1: FDA Compliance Infrastructure - Research

**Researched:** 2026-02-27
**Domain:** SDTM derivation utilities, dataset execution pipeline, XPT compliance
**Confidence:** HIGH

## Summary

Phase 4.1 closes foundational gaps between the existing mapping specification system and submission-ready SDTM output. The codebase has strong foundations: date conversion utilities (dates.py), USUBJID generation (usubjid.py), XPT writer with validation (xpt_writer.py), enriched mapping models (mapping.py), and complete SDTM-IG reference data (domains.json with 26 domains, codelists.json with 27 codelists). What is missing is the "execution bridge" -- the code that takes an approved DomainMappingSpec plus raw DataFrames and produces SDTM-compliant DataFrames ready for XPT writing.

The research covers 14 specific areas: --DY calculation, --SEQ generation, EPOCH derivation, VISITNUM/VISIT assignment, the execution pipeline architecture, sort order enforcement, variable order enforcement, origin metadata, date imputation flags, character length optimization, ASCII validation, missing CT codelists, cross-domain USUBJID validation (already exists), and model extensions.

**Primary recommendation:** Build the execution pipeline as a `DatasetExecutor` class in a new `src/astraea/execution/` module that consumes a DomainMappingSpec and raw DataFrames, applies each VariableMapping pattern deterministically, then runs the derivation utilities (--DY, --SEQ, EPOCH, VISITNUM) as post-processing steps before handing off to the XPT writer.

## Standard Stack

No new libraries needed. All work uses existing stack.

### Core (already installed)
| Library | Purpose | Used For |
|---------|---------|----------|
| `pandas` | DataFrame manipulation | All execution pipeline transforms |
| `pyreadstat` | XPT writing | Already in xpt_writer.py |
| `pydantic` | Model extensions | Origin, imputation flag fields |
| `loguru` | Logging | All new modules |

### No New Dependencies
This phase is entirely about deterministic Python code building on existing infrastructure. No new packages required.

## Architecture Patterns

### Recommended New Module Structure
```
src/astraea/
  execution/              # NEW - dataset execution pipeline
    __init__.py
    executor.py           # DatasetExecutor class
    pattern_handlers.py   # Per-MappingPattern transform functions
  transforms/
    __init__.py           # Update exports
    dates.py              # EXISTS - extend with --DY, imputation flags
    usubjid.py            # EXISTS - no changes needed
    study_day.py          # NEW - --DY calculation
    sequence.py           # NEW - --SEQ generation
    epoch.py              # NEW - EPOCH derivation
    visit.py              # NEW - VISITNUM/VISIT assignment
    ascii_validation.py   # NEW - ASCII checking and cleanup
    char_length.py        # NEW - character length optimization
```

### Pattern 1: DatasetExecutor Pipeline
**What:** A class that orchestrates the full transformation from raw data to SDTM DataFrame.
**When:** After a DomainMappingSpec is approved through human review.
**Flow:**
```
DomainMappingSpec + raw DataFrames
  -> Apply each VariableMapping (pattern_handlers)
  -> Derive --SEQ (always last sequence step)
  -> Derive --DY (needs RFSTDTC from DM)
  -> Derive EPOCH (needs SE domain)
  -> Assign VISITNUM/VISIT (needs TV/SV)
  -> Enforce variable order (from SDTM-IG spec)
  -> Enforce sort order (from domain key_variables)
  -> Optimize character lengths
  -> Validate ASCII
  -> Return SDTM-compliant DataFrame
```

### Pattern 2: Pattern Handler Registry
**What:** A mapping from MappingPattern enum to handler functions.
**When:** During execution, each VariableMapping is dispatched to its pattern handler.
```python
PATTERN_HANDLERS: dict[MappingPattern, Callable] = {
    MappingPattern.ASSIGN: handle_assign,      # Set constant value
    MappingPattern.DIRECT: handle_direct,       # Copy column as-is
    MappingPattern.RENAME: handle_rename,        # Rename column
    MappingPattern.REFORMAT: handle_reformat,    # Apply transform (e.g., date conversion)
    MappingPattern.DERIVATION: handle_derivation, # Compute from logic/rule
    MappingPattern.LOOKUP_RECODE: handle_lookup,  # Map via codelist
    MappingPattern.COMBINE: handle_combine,       # Merge multiple sources
    MappingPattern.SPLIT: handle_split,           # Split one source to many
    MappingPattern.TRANSPOSE: handle_transpose,    # Horizontal to vertical
}
```

### Pattern 3: Cross-Domain Context
**What:** Some derivations need data from other domains (RFSTDTC from DM, SE for EPOCH, TV for VISITNUM).
**When:** --DY, EPOCH, VISITNUM derivations.
**Design:** The DatasetExecutor accepts an optional `cross_domain_context` dict with pre-computed reference data:
```python
@dataclass
class CrossDomainContext:
    rfstdtc_lookup: dict[str, str]  # USUBJID -> RFSTDTC (from DM)
    se_data: pd.DataFrame | None     # SE domain for EPOCH
    tv_data: pd.DataFrame | None     # TV domain for VISITNUM
```

### Anti-Patterns to Avoid
- **LLM-generated derivation code at runtime:** All derivation logic must be deterministic. The LLM proposes WHAT to derive; the code knows HOW.
- **Executing transforms before all mappings are applied:** Some derivations depend on other mapped variables being present. Apply all direct/rename/assign mappings first, then derive.
- **Mutating the original raw DataFrame:** Always work on copies.

## Don't Hand-Roll

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Date parsing | Custom parsers | Existing `dates.py` functions | Already handles SAS DATE, DATETIME, string dates, partial dates with 426 lines of tested code |
| USUBJID generation | String concatenation | Existing `usubjid.py` | Handles NaN detection, whitespace stripping, cross-domain validation |
| XPT validation | Manual checks | Existing `xpt_writer.py` `validate_for_xpt_v5()` | Already validates names, labels, ASCII, byte lengths |
| SDTM-IG variable specs | Hardcoded lists | `SDTMReference.get_domain_spec()` | domains.json has 26 domains with ordered variable specs |
| CT validation | Manual term checking | `CTReference.validate_term()` | Handles extensible vs non-extensible distinction |

## Common Pitfalls

### Pitfall 1: --DY Day-1 Convention (No Day 0)
**What goes wrong:** Calculating study day as simple date difference gives Day 0 for the reference date and negative values that are off by one for pre-treatment dates.
**Why it happens:** SDTM uses a "no Day 0" convention where RFSTDTC itself is Day 1, and the day before is Day -1.
**How to avoid:** Use the exact formula:
```python
if event_date >= ref_date:
    dy = (event_date - ref_date).days + 1   # Day 1 = ref date
else:
    dy = (event_date - ref_date).days        # Day -1 = day before ref
```
**Warning signs:** --DY values of 0 in output; --DY values off by 1 for pre-treatment events.

### Pitfall 2: --DY with Partial Dates
**What goes wrong:** Attempting to calculate --DY when either the event date or RFSTDTC is a partial date (e.g., "2022-03" with no day).
**Why it happens:** Partial dates cannot be compared at day-level precision.
**How to avoid:** --DY must be null/missing when either date lacks day-level precision. Check ISO 8601 string length >= 10 (YYYY-MM-DD) before calculating.

### Pitfall 3: EPOCH Assignment Date Selection
**What goes wrong:** Using the wrong date variable to assign EPOCH per domain class.
**Why it happens:** Different observation classes use different timing variables.
**How to avoid:**
- Findings class: use --DTC (date of test/collection)
- Events class: use --STDTC (start of event)
- Interventions class: use --STDTC (start of intervention)
- If the observation date falls between SESTDTC and SEENDTC for an element, assign that element's EPOCH.

### Pitfall 4: --SEQ Not Monotonically Increasing
**What goes wrong:** --SEQ values have gaps, duplicates, or are not sequential within USUBJID.
**Why it happens:** Sorting is done incorrectly or rows are filtered after SEQ assignment.
**How to avoid:** Always assign --SEQ as the LAST step before output, after all filtering and sorting is complete. Use `groupby(USUBJID).cumcount() + 1`.

### Pitfall 5: Variable Order in XPT Differs from SDTM-IG
**What goes wrong:** P21 flags "variable order does not match define.xml" or variables appear in unexpected order.
**Why it happens:** DataFrame column order is not explicitly managed.
**How to avoid:** After all transforms, reorder columns to match the `order` field from `VariableSpec` in the domain spec. Identifier variables first (STUDYID, DOMAIN, USUBJID, --SEQ), then topic, then qualifiers, then timing.

### Pitfall 6: Character Length Padding to 200
**What goes wrong:** XPT character variables are padded to 200 bytes, making files unnecessarily large and P21 may flag inefficient storage.
**Why it happens:** Default character variable length in pyreadstat is the max observed, but if not explicitly set, it may use 200.
**How to avoid:** Before XPT write, compute max actual length per character column and set `column_widths` parameter in pyreadstat `write_xport()`.

## Code Examples

### --DY Calculation
```python
# Source: SDTM-IG v3.4 timing variable rules
from datetime import date

def calculate_study_day(
    event_dtc: str,
    rfstdtc: str,
) -> int | None:
    """Calculate --DY (study day) from event date and reference start date.

    SDTM convention: RFSTDTC = Day 1, no Day 0.
    Pre-treatment days are negative (-1 is day before RFSTDTC).

    Both dates must have at least day-level precision (YYYY-MM-DD).
    Returns None if either date is partial or missing.
    """
    if not event_dtc or not rfstdtc:
        return None

    # Extract date portion (first 10 chars of ISO 8601)
    event_date_str = event_dtc[:10]
    ref_date_str = rfstdtc[:10]

    # Must have full date precision (YYYY-MM-DD = 10 chars)
    if len(event_date_str) < 10 or len(ref_date_str) < 10:
        return None

    try:
        event_date = date.fromisoformat(event_date_str)
        ref_date = date.fromisoformat(ref_date_str)
    except ValueError:
        return None

    diff = (event_date - ref_date).days

    if diff >= 0:
        return diff + 1  # Day 1 = reference date
    else:
        return diff       # Day -1 = day before reference
```

### --SEQ Generation
```python
# Source: SDTM-IG v3.4 - SEQ is monotonic integer within USUBJID
import pandas as pd

def generate_seq(
    df: pd.DataFrame,
    domain: str,
    sort_keys: list[str],
    usubjid_col: str = "USUBJID",
) -> pd.Series:
    """Generate --SEQ variable: monotonic integer within USUBJID.

    Must be called AFTER sorting and AFTER all filtering is complete.
    """
    seq_col = f"{domain}SEQ"

    # Sort by USUBJID + domain-specific keys
    full_sort = [usubjid_col] + [k for k in sort_keys if k != usubjid_col]
    df_sorted = df.sort_values(full_sort, na_position="last")

    # Assign sequential numbers within each USUBJID
    return df_sorted.groupby(usubjid_col).cumcount() + 1
```

### EPOCH Derivation
```python
# Source: SDTM-IG v3.4, SE domain, EPOCH assignment rules
import pandas as pd

def assign_epoch(
    df: pd.DataFrame,
    se_df: pd.DataFrame,
    date_col: str,  # --DTC for Findings, --STDTC for Events/Interventions
    usubjid_col: str = "USUBJID",
) -> pd.Series:
    """Assign EPOCH based on SE domain date ranges.

    For each record, find the SE element where:
      SESTDTC <= observation_date <= SEENDTC
    and return that element's EPOCH value.
    """
    epochs = pd.Series(index=df.index, dtype="object")

    for idx, row in df.iterrows():
        usubjid = row[usubjid_col]
        obs_date = str(row[date_col])[:10] if pd.notna(row[date_col]) else None

        if not obs_date or len(obs_date) < 10:
            continue

        subject_se = se_df[se_df[usubjid_col] == usubjid].copy()
        for _, se_row in subject_se.iterrows():
            start = str(se_row["SESTDTC"])[:10] if pd.notna(se_row["SESTDTC"]) else None
            end = str(se_row["SEENDTC"])[:10] if pd.notna(se_row["SEENDTC"]) else None

            if start and obs_date >= start:
                if end is None or obs_date <= end:
                    epochs.at[idx] = se_row["EPOCH"]
                    break

    return epochs
```

### Date Imputation Flags
```python
# Source: SDTM-IG v3.4 / ADaM IG date imputation conventions
def get_date_imputation_flag(
    original_dtc: str,
    imputed_dtc: str,
) -> str | None:
    """Determine --DTF (Date Imputation Flag) value.

    --DTF indicates the highest level of date imputation:
      'Y' = year was imputed
      'M' = month was imputed (year present)
      'D' = day was imputed (year and month present)
      None = no imputation performed

    Compare original partial date to imputed full date.
    """
    if not original_dtc or not imputed_dtc:
        return None

    orig_len = len(original_dtc.split("T")[0])  # Date portion only

    if orig_len >= 10:  # YYYY-MM-DD - no date imputation
        return None
    elif orig_len >= 7:  # YYYY-MM - day imputed
        return "D"
    elif orig_len >= 4:  # YYYY - month imputed
        return "M"
    else:
        return "Y"


def get_time_imputation_flag(
    original_dtc: str,
    imputed_dtc: str,
) -> str | None:
    """Determine --TMF (Time Imputation Flag) value.

    --TMF indicates the highest level of time imputation:
      'H' = hour was imputed
      'M' = minute was imputed (hour present)
      'S' = second was imputed (hour and minute present)
      None = no time imputation performed
    """
    if not original_dtc or not imputed_dtc:
        return None

    if "T" not in original_dtc:
        # No time in original but time exists in imputed
        if "T" in imputed_dtc:
            return "H"
        return None

    time_part = original_dtc.split("T")[1] if "T" in original_dtc else ""
    parts = time_part.replace(":", "").strip()

    if len(parts) >= 6:  # HH:MM:SS
        return None
    elif len(parts) >= 4:  # HH:MM
        return "S"
    elif len(parts) >= 2:  # HH
        return "M"
    else:
        return "H"
```

### Execution Pipeline Handler (ASSIGN Pattern)
```python
# Source: Pattern handler for MappingPattern.ASSIGN
import pandas as pd
from astraea.models.mapping import VariableMapping, MappingPattern

def handle_assign(
    df: pd.DataFrame,
    mapping: VariableMapping,
) -> pd.Series:
    """Handle ASSIGN pattern: set constant value for all rows."""
    if mapping.assigned_value is None:
        raise ValueError(
            f"ASSIGN pattern for {mapping.sdtm_variable} has no assigned_value"
        )
    return pd.Series(mapping.assigned_value, index=df.index, dtype="object")


def handle_direct(
    df: pd.DataFrame,
    mapping: VariableMapping,
) -> pd.Series:
    """Handle DIRECT pattern: copy source column to target."""
    if mapping.source_variable is None:
        raise ValueError(
            f"DIRECT pattern for {mapping.sdtm_variable} has no source_variable"
        )
    if mapping.source_variable not in df.columns:
        raise KeyError(
            f"Source variable '{mapping.source_variable}' not found in DataFrame"
        )
    return df[mapping.source_variable].copy()


def handle_rename(
    df: pd.DataFrame,
    mapping: VariableMapping,
) -> pd.Series:
    """Handle RENAME pattern: same as DIRECT (copy with new name)."""
    return handle_direct(df, mapping)
```

### Character Length Optimization
```python
# Source: XPT v5 best practices - minimize character variable widths
import pandas as pd

def optimize_char_lengths(df: pd.DataFrame) -> dict[str, int]:
    """Compute optimal character variable lengths (max observed, minimum 1).

    Returns dict of column_name -> optimal_length for character columns.
    Used as column_widths parameter in pyreadstat.write_xport().
    """
    widths: dict[str, int] = {}
    for col in df.columns:
        if pd.api.types.is_string_dtype(df[col]) or pd.api.types.is_object_dtype(df[col]):
            non_null = df[col].dropna().astype(str)
            if non_null.empty:
                widths[col] = 1  # Minimum width
            else:
                max_len = non_null.str.encode("ascii", errors="replace").str.len().max()
                widths[col] = max(int(max_len), 1)
    return widths
```

## Detailed Research Findings

### 1. --DY (Study Day) Calculation Rules

**Confidence: HIGH** (verified against SDTM-IG v3.4 and multiple authoritative sources)

Formula per SDTM-IG:
- `--DY = (date portion of --DTC) - (date portion of RFSTDTC) + 1` when `--DTC >= RFSTDTC`
- `--DY = (date portion of --DTC) - (date portion of RFSTDTC)` when `--DTC < RFSTDTC`
- Day 1 = RFSTDTC (reference start date from DM domain)
- Day -1 = day before RFSTDTC
- No Day 0 exists
- Both dates must have day-level precision (YYYY-MM-DD minimum)
- --DY is null when either date is partial
- RFSTDTC comes from the DM domain (Reference Start Date/Time of the Subject)

Applies to all --DY suffixed variables: AESTDY, AEENDY, CMSTDY, CMENDY, LBDY, VSDY, etc.

### 2. --SEQ Generation Rules

**Confidence: HIGH** (verified against SDTM-IG v3.4)

- --SEQ is Required (Req) in all general observation class domains
- Must be a positive integer, monotonically increasing within USUBJID
- No gaps allowed (1, 2, 3... not 1, 3, 5)
- Assignment is based on the domain's natural sort order
- Must be assigned AFTER all filtering/deduplication is complete
- STUDYID + DOMAIN + USUBJID + --SEQ forms the surrogate key for each domain

Sort order for SEQ assignment per domain (from key_variables in domains.json):
| Domain | Sort Keys for SEQ |
|--------|------------------|
| DM | STUDYID, USUBJID (one record per subject, SEQ not applicable) |
| AE | STUDYID, USUBJID, AETERM, AESTDTC |
| CM | STUDYID, USUBJID, CMTRT, CMSTDTC |
| EX | STUDYID, USUBJID, EXTRT, EXSTDTC |
| LB | STUDYID, USUBJID, LBTESTCD, LBSPEC, VISITNUM, LBDTC |
| VS | STUDYID, USUBJID, VSTESTCD, VISITNUM, VSDTC |
| DS | STUDYID, USUBJID, DSDECOD, DSSTDTC |
| MH | STUDYID, USUBJID, MHTERM |
| SE | STUDYID, USUBJID, ETCD, SESTDTC |
| SV | STUDYID, USUBJID, VISITNUM |

Note: DM does not have --SEQ (DMSEQ). The DM domain has one record per subject.

### 3. EPOCH Derivation

**Confidence: HIGH** (verified against SDTM-IG v3.4 and Pinnacle 21/Certara guidance)

EPOCH assignment process:
1. Build or receive the SE (Subject Elements) domain with SESTDTC, SEENDTC, EPOCH per element
2. For each record in the target domain, determine the observation date:
   - Findings class (LB, VS, EG, PE, QS): use --DTC
   - Events class (AE, DS, MH, CE, DV): use --STDTC
   - Interventions class (CM, EX): use --STDTC
3. Find the SE element where SESTDTC <= observation_date <= SEENDTC
4. Assign that element's EPOCH value

Typical EPOCH values (from C99079 codelist, already in our codelists.json):
- SCREENING
- RUN-IN (if applicable)
- TREATMENT
- FOLLOW-UP

Key rules:
- SE elements must have NO gaps -- SEENDTC of one element = SESTDTC of the next
- If observation date falls outside all SE elements, EPOCH is null
- EPOCH is Permissible (Perm) in most domains, not Required
- EPOCH uses CT codelist C99079 (extensible -- study-specific values allowed)

### 4. VISITNUM/VISIT Derivation

**Confidence: MEDIUM** (approach varies significantly by sponsor/study)

Two approaches:
1. **From TV domain (planned visits):** Map actual visit dates to planned visits defined in TV (Trial Visits). VISITNUM and VISIT values must match those in TV for planned visits.
2. **From raw data:** Many EDC systems export visit information directly (InstanceName, FolderName in Medidata Rave). Map these to standardized VISIT/VISITNUM values.

Key rules:
- VISITNUM is numeric, VISIT is character
- Planned visits must use values from TV domain
- Unplanned visits get VISITNUM values between planned visits (e.g., 2.1 between visits 2 and 3)
- VISITNUM must be unique within USUBJID for a given domain
- For Findings domains (LB, VS, EG), VISITNUM is part of the natural key

For this project, the raw Medidata Rave data has `InstanceName` and `FolderName` columns that map to visit information. The approach should be:
1. Create a visit mapping lookup (raw visit name -> VISITNUM, VISIT)
2. Apply during execution pipeline

### 5. Variable Origin Metadata

**Confidence: HIGH** (verified against define.xml 2.0 and 2.1 specs)

Valid Origin Type values for define.xml 2.0 (most common for SDTM-IG v3.4):
| Origin Type | Description | Example |
|-------------|-------------|---------|
| CRF | Collected on case report form | AETERM, AESTDTC raw dates |
| Derived | Calculated from other variables | AESTDY, AESEQ, USUBJID, AGE |
| Assigned | Constant value set by sponsor | STUDYID, DOMAIN |
| Protocol | Value from study protocol | ACTARM (if from randomization) |
| eDT | Electronic data transfer (not CRF) | Lab data from central lab |

For define.xml 2.1:
- "CRF" renamed to "Collected"
- "eDT" merged into "Collected"
- Added: Source attribute required alongside Type

Model extension needed for VariableMapping:
```python
class VariableOrigin(StrEnum):
    CRF = "CRF"
    DERIVED = "Derived"
    ASSIGNED = "Assigned"
    PROTOCOL = "Protocol"
    EDT = "eDT"
    PREDECESSOR = "Predecessor"
```

Also need `computational_method: str | None` for define.xml MethodDef -- the textual description of how derived variables are computed.

### 6. Date Imputation Flags (--DTF, --TMF)

**Confidence: HIGH** (verified against ADaM IG which defines the standard)

--DTF (Date Imputation Flag):
- "Y" = year was imputed
- "M" = month was imputed (year present in original)
- "D" = day was imputed (year+month present in original)
- Null = no imputation

--TMF (Time Imputation Flag):
- "H" = hour was imputed
- "M" = minute was imputed
- "S" = second was imputed
- Null = no imputation

Important: In SDTM, dates are generally NOT imputed -- partial dates remain partial in --DTC variables. Imputation flags are more relevant for ADaM. However, if the execution pipeline needs to impute dates for derivation purposes (e.g., --DY calculation with partial dates), the original partial date should be preserved in --DTC and any imputed version stored alongside with imputation flags.

For Phase 4.1, the recommendation is:
- Store original partial dates in --DTC as-is (no imputation)
- --DY is null when --DTC is partial (cannot calculate)
- Build the imputation flag utilities for future use (Phase 5+ for ADaM)

### 7. SDTM Sort Order Rules

**Confidence: HIGH** (from domains.json key_variables, verified against SDTM-IG v3.4)

Sort order = key_variables from domain spec. Already defined in domains.json for all 26 domains.

Standard sort orders:
| Domain | Sort Order |
|--------|-----------|
| DM | STUDYID, USUBJID |
| AE | STUDYID, USUBJID, AETERM, AESTDTC |
| CM | STUDYID, USUBJID, CMTRT, CMSTDTC |
| EX | STUDYID, USUBJID, EXTRT, EXSTDTC |
| LB | STUDYID, USUBJID, LBTESTCD, LBSPEC, VISITNUM, LBDTC |
| VS | STUDYID, USUBJID, VSTESTCD, VISITNUM, VSDTC |
| DS | STUDYID, USUBJID, DSDECOD, DSSTDTC |
| MH | STUDYID, USUBJID, MHTERM |

### 8. SDTM Variable Order

**Confidence: HIGH** (from VariableSpec.order field in domains.json)

Variable order in each domain is defined by the `order` field in VariableSpec. This order must be:
1. Used for DataFrame column ordering before XPT write
2. Reflected in define.xml
3. Standard ordering: identifiers first (STUDYID, DOMAIN, USUBJID, --SEQ), then topic, then qualifiers, then timing

The existing `VariableMapping` model already has an `order` field (added in Phase 3.1 audit fixes). The execution pipeline must sort columns by this order.

### 9. Model Extensions Needed

**VariableMapping additions:**
```python
# Already exists: order, length
# Need to add:
origin: VariableOrigin | None = None          # CRF, Derived, Assigned, Protocol, eDT
computational_method: str | None = None        # For define.xml MethodDef
```

**VariableMappingProposal additions:**
```python
# The LLM can propose origin type:
origin: str | None = None  # Validated and converted to VariableOrigin during enrichment
```

### 10. Missing CT Codelists

**Confidence: MEDIUM** (could not verify exact term lists from NCI EVS due to file size)

The phase description calls for 4 codelists. Current status:

| C-Code | Name | Status | Notes |
|--------|------|--------|-------|
| C101854 | Outcome (of Event) | MISSING | Terms likely: RECOVERED/RESOLVED, RECOVERING/RESOLVING, NOT RECOVERED/NOT RESOLVED, RECOVERED/RESOLVED WITH SEQUELAE, FATAL. Note: C66768 (Outcome of Event) already exists with these terms. C101854 may be a different codelist or may not exist -- needs verification against NCI EVS. |
| C71148 | Position | EXISTS | Already in codelists.json with terms SITTING, STANDING, SUPINE |
| C66785 | Laterality | MISSING | Expected terms: LEFT, RIGHT, BILATERAL. Non-extensible. Used by PE.PELAT and other domains. |
| C66789 | Specimen Condition | MISSING | Expected terms for specimen quality/condition. Used by LB domain. |

Note: C71148 is listed as missing in the phase description but actually already exists. The phase description lists it twice, which appears to be a duplicate.

### 11. Execution Pipeline Design

The execution pipeline transforms an approved DomainMappingSpec + raw DataFrames into an SDTM-compliant DataFrame.

Steps in order:
1. **Merge raw DataFrames** if multiple source datasets
2. **Apply ASSIGN patterns** (STUDYID, DOMAIN constants)
3. **Apply DIRECT/RENAME patterns** (copy/rename columns)
4. **Apply REFORMAT patterns** (date conversions via transform registry)
5. **Apply LOOKUP_RECODE patterns** (codelist mappings)
6. **Apply DERIVATION patterns** (USUBJID, computed fields)
7. **Apply COMBINE patterns** (multi-source derivations)
8. **Derive --DY** (needs RFSTDTC from cross-domain context)
9. **Assign EPOCH** (needs SE domain from cross-domain context)
10. **Assign VISITNUM/VISIT** (needs TV/visit mapping)
11. **Sort by key_variables** (from domain spec)
12. **Generate --SEQ** (must be after sort, after all filtering)
13. **Enforce variable column order** (from VariableSpec.order)
14. **Drop unmapped/extra columns**
15. **Optimize character lengths**
16. **Validate ASCII**
17. **Return SDTM DataFrame** ready for XPT writer

### 12. ASCII Validation

**Confidence: HIGH** (XPT v5 requires ASCII-only)

The existing `validate_for_xpt_v5()` in xpt_writer.py already checks ASCII compliance. For the execution pipeline, add a pre-write step that:
1. Detects non-ASCII characters in string columns
2. Logs specific values and their locations
3. Optionally replaces common non-ASCII characters (curly quotes, en-dash, etc.) with ASCII equivalents
4. Raises an error for truly non-ASCII data that cannot be auto-fixed

### 13. Character Length Optimization

**Confidence: HIGH** (standard XPT best practice)

Before writing XPT:
1. For each character column, compute max observed byte length (ASCII)
2. Set that as the column width (minimum 1)
3. Pass as `column_widths` dict to pyreadstat `write_xport()`
4. This prevents 200-byte padding and reduces file size significantly

Note: pyreadstat's `write_xport()` accepts `column_widths` parameter as a dict mapping column names to widths.

### 14. Cross-Domain USUBJID Validation

**Confidence: HIGH** (already implemented)

The `validate_usubjid_consistency()` function in `transforms/usubjid.py` already handles this:
- Checks all USUBJIDs in non-DM domains exist in DM
- Checks for duplicate USUBJIDs in DM
- Checks USUBJID format consistency

This function should be called as a final validation step after all domains are generated.

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| define.xml 2.0 origin types | define.xml 2.1 origin types | 2021 | "CRF" -> "Collected", "eDT" merged into "Collected" |
| Manual EPOCH assignment | SE-driven EPOCH | SDTM-IG v3.2+ | Systematic, auditable EPOCH derivation |
| P21 Enterprise (commercial) | CDISC CORE (open source) | 2023-2024 | Free validation engine, Python API |

## Open Questions

1. **VISITNUM mapping source:** The raw Medidata Rave data has `InstanceName` and `FolderName` -- how do these map to standardized VISIT/VISITNUM values? This likely needs a study-specific visit mapping table, either manually created or LLM-assisted.
   - Recommendation: Build the utility to accept a visit mapping dict; defer the mapping creation to domain execution time.

2. **C101854 codelist identity:** The phase description references C101854 as "Outcome" but C66768 (Outcome of Event, already in codelists.json) appears to cover the same territory. Need to verify whether C101854 is a distinct codelist or a duplicate reference.
   - Recommendation: Check NCI EVS directly; if C101854 does not exist as an SDTM codelist, remove from scope.

3. **TRANSPOSE pattern execution:** The execution pipeline needs to handle horizontal-to-vertical transpose for Findings domains (LB, VS, EG). This is the most complex pattern and may need domain-specific logic.
   - Recommendation: Implement basic transpose handler in Phase 4.1; enhance per-domain in Phase 5/6.

4. **pyreadstat column_widths parameter:** Need to verify the exact parameter name and behavior for setting character variable widths in `write_xport()`.
   - Recommendation: Test with pyreadstat before implementing.

## Sources

### Primary (HIGH confidence)
- SDTM-IG v3.4 reference data bundled in `/src/astraea/data/sdtm_ig/domains.json` -- domain specs, variable specs, key variables
- CT codelists bundled in `/src/astraea/data/ct/codelists.json` -- 27 codelists verified
- Existing codebase: `transforms/dates.py`, `transforms/usubjid.py`, `io/xpt_writer.py`, `models/mapping.py`
- Phase 3 Audit: `.planning/PHASE3_AUDIT.md` -- comprehensive gap analysis
- SDTM-IG v3.4 Reference: `.planning/research/SDTM_IG_V34_REFERENCE.md`

### Secondary (MEDIUM confidence)
- [Study Day Calculation in SDTM](https://studysas.blogspot.com/2013/01/studyday-calculation-dy-variable-in-sdtm.html) -- --DY formula
- [CDISC Timing Variables Guide](https://www.cdisc.org/kb/articles/when-did-happen-brief-guide-representing-timing-sdtm) -- timing variable rules
- [SDTM TA, TE, and SE Domains (PharmaSUG 2015)](https://pharmasug.org/proceedings/2015/DS/PharmaSUG-2015-DS02.pdf) -- SE/EPOCH rules
- [Assigning VISITNUM and EPOCH (PharmaSUG China 2017)](https://www.lexjansen.com/pharmasug-cn/2017/CD/PharmaSUG-China-2017-CD05_ppt.pdf) -- VISITNUM derivation
- [Define-XML Origin Types (Certara Forum)](https://forum.certara.com/t/dd0073-invalid-origin-type-value/2046) -- origin type values
- [CDISC Define-XML 2.1 Origin Wiki](https://wiki.cdisc.org/display/DEFXML2DOT1/Origin+for+SDTM+Datasets) -- origin definitions
- [sdtm.oak derive_seq (R CRAN)](https://rdrr.io/cran/sdtm.oak/man/derive_seq.html) -- SEQ derivation rules
- [Admiral Date Imputation (R)](https://pharmaverse.github.io/admiral/cran-release/reference/derive_vars_dtm.html) -- DTF/TMF rules
- [SE Subject Element Domain (Medium)](https://fangya.medium.com/sdtm-se-subject-element-domain-6f40c10596b) -- SE domain overview

### Tertiary (LOW confidence)
- C101854, C66785, C66789 codelist term lists -- could not verify exact terms from NCI EVS (file too large to fetch). Terms listed above are based on domain knowledge.
- pyreadstat `column_widths` parameter -- needs verification against current pyreadstat docs.

## Metadata

**Confidence breakdown:**
- --DY calculation: HIGH -- SDTM-IG formula is well-documented and unambiguous
- --SEQ generation: HIGH -- simple monotonic integer, well-defined rules
- EPOCH derivation: HIGH -- SE-based approach is standard
- VISITNUM/VISIT: MEDIUM -- approach varies by sponsor; utility design is clear but mapping logic is study-specific
- Execution pipeline: HIGH -- patterns well-understood, architecture follows existing codebase conventions
- Origin metadata: HIGH -- define.xml origin types well-documented
- Date imputation flags: HIGH -- ADaM IG rules are clear
- Sort/variable order: HIGH -- defined in domains.json
- Missing CT codelists: MEDIUM -- exact term lists need NCI EVS verification
- Character length optimization: HIGH -- standard XPT practice

**Research date:** 2026-02-27
**Valid until:** 2026-03-27 (stable domain -- SDTM-IG v3.4 is not changing)
