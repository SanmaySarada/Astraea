---
phase: 04-human-review-gate
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/astraea/review/__init__.py
  - src/astraea/review/models.py
  - src/astraea/review/session.py
  - tests/unit/review/__init__.py
  - tests/unit/review/test_models.py
  - tests/unit/review/test_session.py
autonomous: true

must_haves:
  truths:
    - "Review session can be created and persisted to SQLite"
    - "Review session can be loaded/resumed from SQLite by session_id"
    - "Human corrections are stored with structured metadata linking original to corrected mapping"
    - "Per-variable review status (pending/approved/corrected/skipped) is tracked"
  artifacts:
    - path: "src/astraea/review/models.py"
      provides: "HumanCorrection, ReviewSession, ReviewDecision, CorrectionType models"
      exports: ["HumanCorrection", "ReviewSession", "ReviewDecision", "CorrectionType", "ReviewStatus", "DomainReviewStatus"]
    - path: "src/astraea/review/session.py"
      provides: "SQLite-backed session persistence"
      exports: ["SessionStore"]
    - path: "tests/unit/review/test_models.py"
      provides: "Model validation tests"
    - path: "tests/unit/review/test_session.py"
      provides: "Session CRUD tests"
  key_links:
    - from: "src/astraea/review/models.py"
      to: "src/astraea/models/mapping.py"
      via: "imports VariableMapping, DomainMappingSpec"
      pattern: "from astraea\\.models\\.mapping import"
    - from: "src/astraea/review/session.py"
      to: "src/astraea/review/models.py"
      via: "imports ReviewSession, HumanCorrection, DomainReviewStatus"
      pattern: "from astraea\\.review\\.models import"
    - from: "src/astraea/review/session.py"
      to: "sqlite3"
      via: "SQLite connection for persistence"
      pattern: "sqlite3\\.connect"
---

<objective>
Create the review data models and SQLite session persistence layer for the human review gate.

Purpose: These are the foundational data structures that all review UI and CLI commands depend on. The models define how corrections are captured (training data for Phase 8 learning). The session store enables interrupt/resume across process exits.
Output: `review/models.py` with Pydantic models, `review/session.py` with SessionStore class, full test coverage.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-human-review-gate/04-RESEARCH.md
@src/astraea/models/mapping.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Review data models</name>
  <files>
    src/astraea/review/__init__.py
    src/astraea/review/models.py
    tests/unit/review/__init__.py
    tests/unit/review/test_models.py
  </files>
  <action>
    Create `src/astraea/review/__init__.py` (empty).
    Create `tests/unit/review/__init__.py` (empty).

    Create `src/astraea/review/models.py` with the following Pydantic models:

    1. `CorrectionType(StrEnum)` with values:
       - SOURCE_CHANGE = "source_change" (different source variable)
       - LOGIC_CHANGE = "logic_change" (different mapping logic)
       - PATTERN_CHANGE = "pattern_change" (different mapping pattern)
       - CT_CHANGE = "ct_change" (different codelist/term)
       - CONFIDENCE_OVERRIDE = "confidence_override" (reviewer disagrees with confidence)
       - REJECT = "reject" (variable should not be mapped)
       - ADD = "add" (variable was missing from proposal)

    2. `ReviewStatus(StrEnum)` with values: PENDING, APPROVED, CORRECTED, SKIPPED

    3. `DomainReviewStatus(StrEnum)` with values: PENDING, IN_PROGRESS, COMPLETED, SKIPPED

    4. `SessionStatus(StrEnum)` with values: IN_PROGRESS, COMPLETED, ABANDONED

    5. `ReviewDecision(BaseModel)`:
       - sdtm_variable: str
       - status: ReviewStatus
       - correction_type: CorrectionType | None = None (only set when status is CORRECTED)
       - original_mapping: VariableMapping (the original proposed mapping)
       - corrected_mapping: VariableMapping | None = None (only set when status is CORRECTED)
       - reason: str = "" (human explanation, required when corrected)
       - timestamp: str (ISO 8601)

    6. `HumanCorrection(BaseModel)`:
       - session_id: str
       - domain: str
       - sdtm_variable: str
       - correction_type: CorrectionType
       - original_mapping: VariableMapping
       - corrected_mapping: VariableMapping | None = None (None for REJECT)
       - reason: str
       - reviewer: str = ""
       - timestamp: str (ISO 8601)

    7. `DomainReview(BaseModel)`:
       - domain: str
       - status: DomainReviewStatus = DomainReviewStatus.PENDING
       - original_spec: DomainMappingSpec
       - reviewed_spec: DomainMappingSpec | None = None (populated after review)
       - decisions: dict[str, ReviewDecision] = {} (keyed by sdtm_variable)
       - corrections: list[HumanCorrection] = []
       - reviewed_at: str | None = None

    8. `ReviewSession(BaseModel)`:
       - session_id: str
       - study_id: str
       - created_at: str (ISO 8601)
       - updated_at: str (ISO 8601)
       - status: SessionStatus = SessionStatus.IN_PROGRESS
       - domains: list[str] (ordered list of domains to review)
       - current_domain_index: int = 0
       - domain_reviews: dict[str, DomainReview] = {} (keyed by domain)

    All models use `from __future__ import annotations`. Import VariableMapping and DomainMappingSpec from `astraea.models.mapping`.

    Create `tests/unit/review/test_models.py`:
    - Test CorrectionType enum values (all 7)
    - Test ReviewStatus enum values (all 4)
    - Test ReviewDecision creation with approve (no correction fields)
    - Test ReviewDecision creation with correction (all fields)
    - Test HumanCorrection round-trip serialization (model_dump_json / model_validate_json)
    - Test ReviewSession creation with defaults
    - Test DomainReview creation with spec, verify decisions dict starts empty
    - Test that corrected_mapping is required when status is CORRECTED (use model_validator)
  </action>
  <verify>
    cd /Users/sanmaysarada/Astraea-SDTM && python -m pytest tests/unit/review/test_models.py -x -q
  </verify>
  <done>All review models importable, enum values correct, serialization round-trips work, validation enforced</done>
</task>

<task type="auto">
  <name>Task 2: SQLite session persistence</name>
  <files>
    src/astraea/review/session.py
    tests/unit/review/test_session.py
  </files>
  <action>
    Create `src/astraea/review/session.py` with a `SessionStore` class:

    ```python
    class SessionStore:
        """SQLite-backed review session persistence.

        Stores review sessions, domain reviews, and corrections in SQLite.
        All writes use transactions. Designed for single-user CLI usage.
        Compatible with future LangGraph SqliteSaver migration.
        """

        def __init__(self, db_path: Path) -> None:
            """Open or create the SQLite database."""
            # Create parent directory if needed
            # Connect with check_same_thread=False for safety
            # Call _ensure_tables()

        def _ensure_tables(self) -> None:
            """Create tables if they don't exist."""
            # sessions table: session_id TEXT PK, study_id TEXT, created_at TEXT,
            #   updated_at TEXT, status TEXT, current_domain_index INTEGER, domains_json TEXT
            # domain_reviews table: session_id TEXT, domain TEXT, status TEXT,
            #   original_spec_json TEXT, reviewed_spec_json TEXT, decisions_json TEXT,
            #   corrections_json TEXT, reviewed_at TEXT, PK(session_id, domain),
            #   FK(session_id)
            # corrections table: id INTEGER PK AUTOINCREMENT, session_id TEXT,
            #   domain TEXT, sdtm_variable TEXT, correction_type TEXT,
            #   original_json TEXT, corrected_json TEXT, reason TEXT,
            #   reviewer TEXT, created_at TEXT, FK(session_id)

        def create_session(self, study_id: str, domains: list[str],
                           specs: dict[str, DomainMappingSpec]) -> ReviewSession:
            """Create a new review session. Generates uuid4 hex[:12] as session_id.
            Stores each domain's spec as a pending DomainReview.
            Returns the ReviewSession object."""

        def save_session(self, session: ReviewSession) -> None:
            """Persist full session state (updated_at, current_domain_index, status).
            Also saves all domain_reviews and corrections."""

        def save_domain_review(self, session_id: str, domain_review: DomainReview) -> None:
            """Save/update a single domain review within a session."""

        def save_correction(self, correction: HumanCorrection) -> None:
            """Append a correction to the corrections table."""

        def load_session(self, session_id: str) -> ReviewSession:
            """Load full session state including all domain reviews.
            Raises ValueError if session_id not found."""

        def list_sessions(self, study_id: str | None = None) -> list[dict]:
            """Return list of session summaries (id, study, status, created, updated, domains count).
            Optionally filter by study_id."""

        def close(self) -> None:
            """Close the database connection."""
    ```

    Use `json` for serializing Pydantic models to/from SQLite TEXT columns via `model_dump_json()` / `Model.model_validate_json()`.

    Use `datetime.now(tz=UTC).isoformat()` for timestamps (import UTC from datetime).

    All writes wrapped in `self._conn.execute()` + `self._conn.commit()`. Use `executescript()` for table creation only.

    Create `tests/unit/review/test_session.py`:
    - Use `tmp_path` fixture for SQLite database path
    - Test create_session returns valid ReviewSession with correct session_id format (12 hex chars)
    - Test save_session + load_session round-trip (session fields match)
    - Test save_domain_review updates domain review in loaded session
    - Test save_correction persists and is retrievable via load_session
    - Test list_sessions returns correct summaries
    - Test list_sessions with study_id filter
    - Test load_session raises ValueError for unknown session_id
    - Test multiple sessions for same study
    - Build test fixtures using real-ish DomainMappingSpec objects (use minimal valid specs with 2-3 VariableMapping entries)
  </action>
  <verify>
    cd /Users/sanmaysarada/Astraea-SDTM && python -m pytest tests/unit/review/test_session.py -x -q
  </verify>
  <done>SessionStore creates/saves/loads/lists sessions correctly, corrections persisted, round-trip serialization verified, all tests pass</done>
</task>

</tasks>

<verification>
cd /Users/sanmaysarada/Astraea-SDTM && python -m pytest tests/unit/review/ -x -q
cd /Users/sanmaysarada/Astraea-SDTM && ruff check src/astraea/review/ tests/unit/review/
cd /Users/sanmaysarada/Astraea-SDTM && python -c "from astraea.review.models import HumanCorrection, ReviewSession, CorrectionType; print('Models OK')"
cd /Users/sanmaysarada/Astraea-SDTM && python -c "from astraea.review.session import SessionStore; print('SessionStore OK')"
</verification>

<success_criteria>
- All review models importable with correct fields and enum values
- SessionStore creates SQLite database, persists sessions, loads them back
- Corrections round-trip through JSON serialization
- All tests pass, ruff clean
</success_criteria>

<output>
After completion, create `.planning/phases/04-human-review-gate/04-01-SUMMARY.md`
</output>
