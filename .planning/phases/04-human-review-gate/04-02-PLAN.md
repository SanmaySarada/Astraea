---
phase: 04-human-review-gate
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - src/astraea/review/display.py
  - src/astraea/review/reviewer.py
  - tests/unit/review/test_display.py
  - tests/unit/review/test_reviewer.py
autonomous: true

must_haves:
  truths:
    - "Reviewer can see a formatted table of proposed mappings with status column and color-coded confidence"
    - "Reviewer can batch-approve all HIGH confidence mappings with one action"
    - "Reviewer can review MEDIUM/LOW confidence mappings one at a time"
    - "Reviewer can correct a mapping by changing the source variable or rejecting it"
    - "Review progress is saved after each variable decision (survives Ctrl+C)"
  artifacts:
    - path: "src/astraea/review/display.py"
      provides: "Review-specific Rich display functions"
      exports: ["display_review_table", "display_variable_detail", "display_review_summary"]
    - path: "src/astraea/review/reviewer.py"
      provides: "Core review loop logic"
      exports: ["DomainReviewer", "ReviewInterrupted"]
    - path: "tests/unit/review/test_display.py"
      provides: "Display rendering tests"
    - path: "tests/unit/review/test_reviewer.py"
      provides: "Review logic tests with mocked input"
  key_links:
    - from: "src/astraea/review/reviewer.py"
      to: "src/astraea/review/session.py"
      via: "saves decisions after each variable"
      pattern: "session_store\\.save_domain_review"
    - from: "src/astraea/review/reviewer.py"
      to: "src/astraea/review/display.py"
      via: "calls display functions for review UI"
      pattern: "display_review_table|display_variable_detail"
    - from: "src/astraea/review/display.py"
      to: "src/astraea/models/mapping.py"
      via: "renders DomainMappingSpec and VariableMapping"
      pattern: "from astraea\\.models\\.mapping import"
---

<objective>
Build the review display layer and core reviewer logic that drives the interactive human review loop.

Purpose: This is the actual review experience -- displaying mappings, collecting approve/correct/skip decisions, and persisting progress after each decision. The two-tier review pattern (batch HIGH, individual MEDIUM/LOW) matches statistical programmer workflow.
Output: `review/display.py` with Rich display functions, `review/reviewer.py` with DomainReviewer class, full test coverage.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-human-review-gate/04-RESEARCH.md
@.planning/phases/04-human-review-gate/04-01-SUMMARY.md
@src/astraea/models/mapping.py
@src/astraea/cli/display.py
@src/astraea/review/models.py
@src/astraea/review/session.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Review display functions</name>
  <files>
    src/astraea/review/display.py
    tests/unit/review/test_display.py
  </files>
  <action>
    Create `src/astraea/review/display.py` extending the patterns from `cli/display.py`:

    1. `display_review_table(spec: DomainMappingSpec, decisions: dict[str, ReviewDecision], console: Console) -> None`:
       - Shows domain header panel (domain, study, source datasets, timestamp)
       - Table columns: #, Status, Variable, Label, Core, Source, Pattern, Confidence, Logic
       - Status column shows: "..." (dim) for pending, "OK" (bold green) for approved, "FIX" (bold yellow) for corrected, "--" (dim) for skipped
       - Core color-coded: Req=red, Exp=yellow, Perm=green (reuse _format_core pattern)
       - Confidence color-coded: HIGH=green, MEDIUM=yellow, LOW=red
       - Source shows source_variable, or ="value" for ASSIGN pattern

    2. `display_variable_detail(mapping: VariableMapping, console: Console) -> None`:
       - Panel showing full detail for one variable:
         - Variable name (bold cyan), Label, Core
         - Source: dataset.variable (or "Assigned: value")
         - Pattern, Confidence (score + level)
         - Mapping Logic (full text, not truncated)
         - Derivation Rule (if present)
         - Codelist (code + name, if present)
         - Rationale (confidence_rationale)

    3. `display_review_summary(domain_review: DomainReview, console: Console) -> None`:
       - Panel showing: domain, total variables, approved count, corrected count, skipped count, pending count
       - List corrections made (variable -> correction_type: reason)

    4. `display_session_list(sessions: list[dict], console: Console) -> None`:
       - Table with: Session ID, Study, Status, Created, Updated, Domains
       - Status color-coded: in_progress=yellow, completed=green, abandoned=red

    Use `from __future__ import annotations`. Import from `astraea.review.models` and `astraea.models.mapping`.

    Create `tests/unit/review/test_display.py`:
    - Use `Console(file=io.StringIO())` to capture output (established pattern from existing display tests)
    - Build minimal DomainMappingSpec + VariableMapping fixtures (2-3 variables with different confidence levels)
    - Test display_review_table renders all columns (check string contains variable names, status indicators)
    - Test display_variable_detail shows full mapping info
    - Test display_review_summary shows correct counts
    - Test display_session_list renders session table
    - Strip ANSI escape codes with regex before assertions (pattern from D-0304-01: `re.sub(r'\x1b\[[0-9;]*m', '', text)`)
  </action>
  <verify>
    cd /Users/sanmaysarada/Astraea-SDTM && python -m pytest tests/unit/review/test_display.py -x -q
  </verify>
  <done>All display functions render correct output, tests capture and verify Rich output via StringIO</done>
</task>

<task type="auto">
  <name>Task 2: Core reviewer logic</name>
  <files>
    src/astraea/review/reviewer.py
    tests/unit/review/test_reviewer.py
  </files>
  <action>
    Create `src/astraea/review/reviewer.py` with the DomainReviewer class:

    ```python
    class ReviewInterrupted(Exception):
        """Raised when reviewer quits mid-session."""
        def __init__(self, session_id: str) -> None:
            self.session_id = session_id
            super().__init__(f"Review interrupted. Resume with: astraea resume {session_id}")

    class DomainReviewer:
        """Drives the interactive review loop for a single domain.

        Supports two review modes:
        1. Two-tier: batch approve HIGH confidence, individual review MEDIUM/LOW
        2. Per-variable: review every variable individually

        Accepts an optional input_callback for testability (replaces Rich Prompt.ask).
        When input_callback is None, uses Rich Prompt.ask() for real terminal input.
        """

        def __init__(
            self,
            session_store: SessionStore,
            console: Console,
            *,
            input_callback: Callable[[str, list[str], str], str] | None = None,
        ) -> None:
            ...
    ```

    Key methods:

    1. `review_domain(self, session_id: str, domain: str) -> DomainReview`:
       - Load domain review from session store
       - Display the full review table (current state)
       - Prompt for action: "approve-all" / "review" / "skip" / "quit"
         - "approve-all": approve every pending variable, save, return
         - "review": call _review_two_tier()
         - "skip": mark domain SKIPPED, save, return
         - "quit": save current state, raise ReviewInterrupted
       - After all variables reviewed, mark domain COMPLETED, save, return

    2. `_review_two_tier(self, session_id: str, domain_review: DomainReview) -> None`:
       - Partition pending variables into HIGH confidence and MEDIUM/LOW
       - If HIGH variables exist, prompt: "Batch approve N HIGH confidence mappings? [Y/n]"
         - Y: approve all HIGH, save after batch
         - n: fall through to per-variable
       - For each MEDIUM/LOW variable (and non-batch-approved HIGH):
         - Display variable detail
         - Prompt: [a]pprove / [c]orrect / [s]kip / [q]uit
         - "a": create ReviewDecision(APPROVED), save immediately
         - "c": call _collect_correction(), create ReviewDecision(CORRECTED), save
         - "s": create ReviewDecision(SKIPPED), save
         - "q": save current state, raise ReviewInterrupted

    3. `_collect_correction(self, mapping: VariableMapping) -> tuple[VariableMapping, CorrectionType, str]`:
       - Prompt correction type: [s]ource / [r]eject / [o]ther
         - "s" (source_change): Prompt for new source variable name, create corrected VariableMapping with updated source_variable
         - "r" (reject): Return (None, REJECT, reason)
         - "o" (other): Prompt for free-text reason, return (original unchanged, LOGIC_CHANGE, reason)
       - Prompt for reason (required, non-empty)
       - Return (corrected_mapping or None, correction_type, reason)

    4. `_prompt(self, message: str, choices: list[str], default: str) -> str`:
       - If self._input_callback is set, call it
       - Otherwise, use `rich.prompt.Prompt.ask(message, choices=choices, default=default)`

    CRITICAL: After every individual variable decision, call `session_store.save_domain_review()` so progress survives Ctrl+C. This is the per-variable persistence from Pitfall 2 in the research.

    When creating corrected VariableMapping for source_change:
    - Copy all fields from original mapping
    - Replace source_variable with the new value
    - Set confidence to 1.0, confidence_level to HIGH (human-verified)
    - Update mapping_logic to note the correction

    Create `tests/unit/review/test_reviewer.py`:
    - Use tmp_path for SessionStore, Console(file=io.StringIO()) for output
    - Use input_callback to simulate user input sequences
    - Test approve-all flow: all variables become APPROVED
    - Test two-tier flow: HIGH batch approved, MEDIUM gets individual review
    - Test correction flow: source_change creates corrected mapping with new source
    - Test reject flow: creates correction with REJECT type, no corrected_mapping
    - Test quit flow: raises ReviewInterrupted, session state is saved
    - Test resume flow: previously approved variables are skipped on re-entry
    - Build helper to create realistic DomainMappingSpec fixtures with mixed confidence levels (3 HIGH, 2 MEDIUM, 1 LOW)
  </action>
  <verify>
    cd /Users/sanmaysarada/Astraea-SDTM && python -m pytest tests/unit/review/test_reviewer.py -x -q
  </verify>
  <done>DomainReviewer handles all review flows (approve-all, two-tier, correction, reject, quit/resume), saves after each decision, all tests pass with mocked input</done>
</task>

</tasks>

<verification>
cd /Users/sanmaysarada/Astraea-SDTM && python -m pytest tests/unit/review/ -x -q
cd /Users/sanmaysarada/Astraea-SDTM && ruff check src/astraea/review/ tests/unit/review/
cd /Users/sanmaysarada/Astraea-SDTM && python -c "from astraea.review.reviewer import DomainReviewer, ReviewInterrupted; print('Reviewer OK')"
</verification>

<success_criteria>
- Review table displays mappings with status, confidence color-coding, and core designation
- DomainReviewer supports approve-all, two-tier (batch HIGH + individual MEDIUM/LOW), and per-variable modes
- Corrections capture structured metadata (type, original, corrected, reason)
- Progress persists after every variable decision (survives Ctrl+C)
- Resume skips already-reviewed variables
- All tests pass with mocked input_callback
</success_criteria>

<output>
After completion, create `.planning/phases/04-human-review-gate/04-02-SUMMARY.md`
</output>
