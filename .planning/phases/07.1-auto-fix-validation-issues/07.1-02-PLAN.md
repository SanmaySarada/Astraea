---
phase: 07.1-auto-fix-validation-issues
plan: 02
type: execute
wave: 2
depends_on: ["07.1-01"]
files_modified:
  - src/astraea/validation/fix_loop.py
  - tests/unit/validation/test_fix_loop.py
autonomous: true

must_haves:
  truths:
    - "System runs validate-fix-revalidate loop up to max 3 iterations"
    - "Loop stops early when no more auto-fixable issues remain"
    - "Each iteration produces a report of what was fixed and what remains"
    - "Needs-human issues are presented with context and suggested fixes"
    - "Fixed DataFrames are written back to XPT files"
    - "All fix actions across iterations are accumulated in a single audit trail"
  artifacts:
    - path: "src/astraea/validation/fix_loop.py"
      provides: "FixLoopEngine with run_fix_loop() method, FixLoopResult model"
      min_lines: 150
    - path: "tests/unit/validation/test_fix_loop.py"
      provides: "Unit tests for loop iteration, early exit, max iterations, result aggregation"
      min_lines: 100
  key_links:
    - from: "src/astraea/validation/fix_loop.py"
      to: "src/astraea/validation/autofix.py"
      via: "uses AutoFixer.apply_fixes()"
      pattern: "from astraea\\.validation\\.autofix import"
    - from: "src/astraea/validation/fix_loop.py"
      to: "src/astraea/validation/engine.py"
      via: "uses ValidationEngine.validate_domain()"
      pattern: "from astraea\\.validation\\.engine import"
    - from: "src/astraea/validation/fix_loop.py"
      to: "src/astraea/io/xpt_writer.py"
      via: "writes fixed DataFrames back to XPT"
      pattern: "from astraea\\.io\\.xpt_writer import|pyreadstat\\.write_xport"
---

<objective>
Build the validate-fix-revalidate loop engine that orchestrates the auto-fix cycle: validate all domains, apply auto-fixes, re-validate to confirm fixes worked, repeat up to 3 iterations. Produces a comprehensive result with accumulated fix actions, remaining issues, and human-action items.

Purpose: This is the orchestration layer that makes auto-fixing practical -- a single call that runs the full cycle and produces an actionable report.
Output: `src/astraea/validation/fix_loop.py` with FixLoopEngine, `tests/unit/validation/test_fix_loop.py`
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07.1-auto-fix-validation-issues/07.1-01-SUMMARY.md

@src/astraea/validation/autofix.py
@src/astraea/validation/engine.py
@src/astraea/validation/report.py
@src/astraea/validation/rules/base.py
@src/astraea/io/xpt_writer.py
@src/astraea/cli/app.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: FixLoopEngine with validate-fix-revalidate cycle</name>
  <files>src/astraea/validation/fix_loop.py</files>
  <action>
Create `src/astraea/validation/fix_loop.py` with these components:

1. **IterationResult Pydantic model**:
   - `iteration: int` -- 1-indexed iteration number
   - `issues_found: int` -- total issues found in this iteration's validation
   - `auto_fixed: int` -- number of issues auto-fixed in this iteration
   - `remaining_auto_fixable: int` -- auto-fixable issues not yet addressed (should decrease)
   - `needs_human: int` -- issues requiring human intervention
   - `fix_actions: list[FixAction]` -- fixes applied in this iteration

2. **FixLoopResult Pydantic model**:
   - `iterations_run: int` -- how many iterations were executed
   - `max_iterations: int` -- configured max (default 3)
   - `converged: bool` -- True if loop stopped because no more auto-fixable issues
   - `total_fixed: int` -- total fixes across all iterations
   - `remaining_issues: list[RuleResult]` -- issues that still exist after all iterations
   - `needs_human_issues: list[IssueClassification]` -- classified needs-human issues with context
   - `all_fix_actions: list[FixAction]` -- accumulated audit trail from all iterations
   - `iteration_details: list[IterationResult]` -- per-iteration breakdown
   - `final_report: ValidationReport` -- the validation report after the last iteration

3. **FixLoopEngine class**:
   - `__init__(self, *, engine: ValidationEngine, auto_fixer: AutoFixer, max_iterations: int = 3)` -- stores engine, auto_fixer, max_iterations
   - `run_fix_loop(self, domains: dict[str, tuple[pd.DataFrame, DomainMappingSpec]], *, output_dir: Path | None = None, study_id: str = "UNKNOWN") -> FixLoopResult`:

    The loop logic:
    ```
    all_fix_actions = []
    iteration_details = []

    for iteration in range(1, max_iterations + 1):
        # Step 1: Validate all domains
        results = engine.validate_all(domains)

        # Step 2: Classify all issues
        classifications = [auto_fixer.classify_issue(r) for r in results]
        auto_fixable = [c for c in classifications if c.classification == AUTO_FIXABLE]
        needs_human = [c for c in classifications if c.classification == NEEDS_HUMAN]

        # Step 3: If no auto-fixable issues, stop
        if not auto_fixable:
            # Record iteration details and break
            break

        # Step 4: Apply fixes per domain
        iteration_fix_actions = []
        for domain_code, (df, spec) in domains.items():
            domain_issues = [c.result for c in auto_fixable if c.result.domain == domain_code]
            if not domain_issues:
                continue
            fixed_df, fix_actions = auto_fixer.apply_fixes(domain_code, df, spec, domain_issues)
            domains[domain_code] = (fixed_df, spec)  # update in-place in the dict
            iteration_fix_actions.extend(fix_actions)

        all_fix_actions.extend(iteration_fix_actions)

        # Record iteration detail
        iteration_details.append(IterationResult(
            iteration=iteration,
            issues_found=len(results),
            auto_fixed=len(iteration_fix_actions),
            remaining_auto_fixable=len(auto_fixable) - len(iteration_fix_actions),
            needs_human=len(needs_human),
            fix_actions=iteration_fix_actions,
        ))

        logger.info("Iteration {}: fixed {} issues, {} remain", iteration, len(iteration_fix_actions), len(needs_human))

    # Step 5: Final validation pass
    final_results = engine.validate_all(domains)
    final_report = ValidationReport.from_results(study_id, final_results, list(domains.keys()))

    # Step 6: Write fixed DataFrames to XPT if output_dir given
    if output_dir is not None:
        _write_fixed_datasets(domains, output_dir)

    # Step 7: Write audit trail to JSON
    if output_dir is not None:
        _write_audit_trail(all_fix_actions, output_dir)

    return FixLoopResult(...)
    ```

4. **Helper function `_write_fixed_datasets`**:
   - For each domain in the dict, write the DataFrame to `{output_dir}/{domain.lower()}.xpt` using pyreadstat.write_xport.
   - Use the spec's variable_mappings for column labels (build column_labels dict from spec).
   - Log each file written.

5. **Helper function `_write_audit_trail`**:
   - Write all FixActions to `{output_dir}/autofix_audit.json` as a JSON array.
   - Each FixAction serialized via model_dump().

6. **Helper function `format_needs_human_report`**:
   - Takes list[IssueClassification] (needs-human only).
   - Returns a formatted string suitable for CLI display.
   - Groups by domain, shows rule_id, variable, message, and suggested_fix.
   - This is what gets displayed to the user so they know what to fix manually.

IMPORTANT: Use `loguru` for all logging. Log each iteration start/end, each fix applied, each file written.
IMPORTANT: The loop updates the domains dict in-place (replacing DataFrames). This is intentional -- each iteration builds on the previous fixes.
IMPORTANT: Handle the case where a fix doesn't actually resolve the issue (prevents infinite loop). The max_iterations cap handles this.
  </action>
  <verify>
  Run `python -c "from astraea.validation.fix_loop import FixLoopEngine, FixLoopResult, IterationResult"` -- imports clean.
  Run `ruff check src/astraea/validation/fix_loop.py` -- passes.
  </verify>
  <done>FixLoopEngine runs validate-fix-revalidate loop up to 3 iterations, accumulates fix actions, writes fixed XPTs and audit trail JSON, produces FixLoopResult with full breakdown.</done>
</task>

<task type="auto">
  <name>Task 2: Unit tests for FixLoopEngine</name>
  <files>tests/unit/validation/test_fix_loop.py</files>
  <action>
Create `tests/unit/validation/test_fix_loop.py` with tests:

1. **test_loop_converges_when_no_auto_fixable**: Pass domains with only needs-human issues. Verify iterations_run=1, converged=True, total_fixed=0.

2. **test_loop_fixes_and_revalidates**: Create a DataFrame with a fixable issue (e.g., missing DOMAIN column for AE domain). Run loop. Verify:
   - iterations_run >= 1
   - total_fixed >= 1
   - DOMAIN column exists in the returned domains dict
   - Fix action has fix_type="add_missing_column" or "fix_domain_value"

3. **test_loop_max_iterations_cap**: Mock the auto_fixer to always return fixes (simulating non-convergence). Verify iterations_run == max_iterations.

4. **test_loop_accumulates_fix_actions**: Run loop with multiple fixable issues across 2+ iterations. Verify all_fix_actions contains fixes from all iterations.

5. **test_loop_needs_human_in_result**: Include both auto-fixable and needs-human issues. Verify needs_human_issues in the result contains the correct items with context.

6. **test_iteration_details_populated**: Verify each IterationResult has correct counts for issues_found, auto_fixed, needs_human.

7. **test_final_report_reflects_fixes**: After loop, the final_report should show fewer errors than the initial validation.

8. **test_format_needs_human_report**: Call format_needs_human_report() with sample IssueClassifications. Verify output string contains domain names, rule IDs, and suggested fixes.

For these tests, use real ValidationEngine + AutoFixer with real bundled reference data (SDTMReference, CTReference). Create minimal DataFrames with known fixable issues. Use tmp_path for output_dir.

Create a pytest fixture `fix_loop_engine` that instantiates ValidationEngine and AutoFixer with real references, then creates FixLoopEngine.

Create a fixture `sample_domain_with_issues` that returns a dict with one domain (e.g., "AE") having a DataFrame missing the DOMAIN column and with a spec.
  </action>
  <verify>
  Run `pytest tests/unit/validation/test_fix_loop.py -v` -- all tests pass.
  Run `ruff check tests/unit/validation/test_fix_loop.py` -- clean.
  </verify>
  <done>8+ tests verify loop convergence, max iteration cap, fix accumulation, needs-human separation, iteration details, and final report correctness.</done>
</task>

</tasks>

<verification>
- `pytest tests/unit/validation/test_fix_loop.py -v` -- all pass
- `ruff check src/astraea/validation/fix_loop.py tests/unit/validation/test_fix_loop.py` -- clean
- FixLoopEngine.run_fix_loop() runs 1-3 iterations and stops when converged
- Audit trail JSON written to output_dir
- Fixed XPT files written to output_dir
</verification>

<success_criteria>
1. Validate-fix-revalidate loop runs up to 3 iterations
2. Loop stops early when no auto-fixable issues remain
3. FixLoopResult includes: iterations_run, total_fixed, remaining_issues, needs_human_issues, all_fix_actions, final_report
4. Needs-human issues include context and suggested fixes
5. Fixed DataFrames written back to XPT files
6. Audit trail with before/after values written to JSON
</success_criteria>

<output>
After completion, create `.planning/phases/07.1-auto-fix-validation-issues/07.1-02-SUMMARY.md`
</output>
