---
phase: 07.1-auto-fix-validation-issues
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/astraea/validation/autofix.py
  - tests/unit/validation/test_autofix.py
autonomous: true

must_haves:
  truths:
    - "Each RuleResult is classified as auto-fixable or needs-human based on its rule_id and content"
    - "Auto-fixable fixes are applied to the DataFrame in-place with before/after audit trail"
    - "CT case normalization fixes wrong-case CT values to correct case from codelist"
    - "Missing DOMAIN/STUDYID columns are added with correct constant values"
    - "Variable labels exceeding 40 chars are truncated to 40 chars"
    - "Variable names exceeding 8 chars are truncated to 8 chars"
    - "Non-ASCII characters in string columns are replaced with ASCII equivalents"
  artifacts:
    - path: "src/astraea/validation/autofix.py"
      provides: "AutoFixer class with classify_issue() and apply_fix() methods, FixAction audit model"
      min_lines: 200
    - path: "tests/unit/validation/test_autofix.py"
      provides: "Unit tests for issue classification, each fix type, and audit trail"
      min_lines: 150
  key_links:
    - from: "src/astraea/validation/autofix.py"
      to: "src/astraea/validation/rules/base.py"
      via: "imports RuleResult, RuleSeverity, RuleCategory"
      pattern: "from astraea\\.validation\\.rules\\.base import"
    - from: "src/astraea/validation/autofix.py"
      to: "src/astraea/reference/controlled_terms.py"
      via: "CT lookups for case normalization"
      pattern: "CTReference"
---

<objective>
Build the AutoFixer core: issue classification (auto-fixable vs needs-human), individual fix functions for each deterministic category, FixAction audit model for before/after logging, and comprehensive unit tests.

Purpose: This is the foundation component that decides what CAN be auto-fixed and HOW. Each fix function is a pure, deterministic transformation on a DataFrame + metadata.
Output: `src/astraea/validation/autofix.py` with AutoFixer class, `tests/unit/validation/test_autofix.py`
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@src/astraea/validation/rules/base.py
@src/astraea/validation/engine.py
@src/astraea/validation/report.py
@src/astraea/validation/rules/terminology.py
@src/astraea/validation/rules/presence.py
@src/astraea/validation/rules/limits.py
@src/astraea/validation/rules/format.py
@src/astraea/reference/controlled_terms.py
@src/astraea/transforms/ascii_validation.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: FixAction audit model and AutoFixer core with issue classification</name>
  <files>src/astraea/validation/autofix.py</files>
  <action>
Create `src/astraea/validation/autofix.py` with these components:

1. **FixAction Pydantic model** (audit trail):
   - `rule_id: str` -- which rule triggered this fix
   - `domain: str` -- domain being fixed
   - `variable: str | None` -- variable being fixed
   - `fix_type: str` -- one of: "ct_case_normalize", "add_missing_column", "truncate_label", "truncate_name", "fix_ascii", "fix_domain_value", "fix_date_format"
   - `before_value: str` -- value/state before fix
   - `after_value: str` -- value/state after fix
   - `affected_count: int` -- number of rows affected
   - `timestamp: str` -- ISO 8601 timestamp

2. **FixClassification enum** (StrEnum):
   - `AUTO_FIXABLE` = "auto_fixable"
   - `NEEDS_HUMAN` = "needs_human"

3. **IssueClassification Pydantic model**:
   - `result: RuleResult`
   - `classification: FixClassification`
   - `reason: str` -- why it's classified this way
   - `suggested_fix: str | None` -- for needs-human, provide context

4. **AutoFixer class**:
   - `__init__(self, *, ct_ref: CTReference, sdtm_ref: SDTMReference)` -- stores references
   - `classify_issue(self, result: RuleResult) -> IssueClassification` -- classifies a single RuleResult
   - `apply_fixes(self, domain: str, df: pd.DataFrame, spec: DomainMappingSpec, issues: list[RuleResult]) -> tuple[pd.DataFrame, list[FixAction]]` -- applies all auto-fixable fixes, returns modified df and audit trail

**Issue classification rules** (in `classify_issue`):

AUTO_FIXABLE rules (map rule_id to fix function):
- `ASTR-T001` (CT value mismatch): Auto-fixable ONLY if the issue is case mismatch (e.g., "male" vs "MALE" for non-extensible codelist). Check by comparing value.upper() against codelist terms (uppercased). If it's a genuinely wrong value (not case issue), classify as NEEDS_HUMAN.
- `ASTR-T002` (DOMAIN column wrong/missing): Always auto-fixable. Fix: add or correct DOMAIN column with the domain code.
- `ASTR-P001` (Required variable missing): Auto-fixable ONLY for STUDYID, DOMAIN, USUBJID -- these have deterministic values (constant, constant, derivable). All other missing required vars -> NEEDS_HUMAN.
- `ASTR-L001` (Variable name >8 chars): Auto-fixable. Truncate column name to 8 chars.
- `ASTR-L002` (Variable label >40 chars): Auto-fixable. Truncate label in spec to 40 chars.
- `ASTR-L003` (Character value >200 bytes): NEEDS_HUMAN (truncating data values is lossy and may require SUPPQUAL split).
- `ASTR-F001` (Date format): NEEDS_HUMAN (date conversion requires understanding the source format -- not deterministic from the error alone).
- `ASTR-F002` (ASCII): Auto-fixable. Use `fix_common_non_ascii()` from transforms.ascii_validation.
- `ASTR-F003` (File naming): Auto-fixable as metadata note only (file rename tracked in FixAction but actual rename deferred to loop engine).
- All FDA Business Rules (FDAB*): NEEDS_HUMAN (require domain expertise).
- All cross-domain rules (ASTR-C*): NEEDS_HUMAN (require multi-domain context).
- Everything else: NEEDS_HUMAN.

5. **Individual fix functions** (private methods on AutoFixer):

- `_fix_ct_case(self, domain, df, result) -> tuple[pd.DataFrame, list[FixAction]]`:
  Look up the codelist from the spec's variable mapping. For each invalid value that matches a codelist term case-insensitively, replace with the correct-case term. Return fix actions with before/after.

- `_fix_domain_column(self, domain, df, result) -> tuple[pd.DataFrame, list[FixAction]]`:
  If DOMAIN column missing, add it with value = domain code. If DOMAIN column has wrong values, replace all with domain code.

- `_fix_missing_studyid(self, domain, df, spec) -> tuple[pd.DataFrame, list[FixAction]]`:
  Add STUDYID column with value from spec.study_id.

- `_fix_variable_name_length(self, df, result) -> tuple[pd.DataFrame, list[FixAction]]`:
  Rename the offending column to its first 8 characters. If collision, append a digit.

- `_fix_variable_label_length(self, spec, result) -> tuple[DomainMappingSpec, list[FixAction]]`:
  Truncate the label in the spec's variable_mappings to 40 chars. Note: this modifies spec, not df.

- `_fix_ascii(self, domain, df, result) -> tuple[pd.DataFrame, list[FixAction]]`:
  Use `fix_common_non_ascii()` from `astraea.transforms.ascii_validation`. Record affected columns/counts.

IMPORTANT: Every fix function returns a COPY of the DataFrame (or spec), never modifies in place. The `apply_fixes` method orchestrates: classify each issue, call the appropriate fix function for auto-fixable ones, collect all FixActions.

IMPORTANT: Use `loguru` logger for all debug/info logging about fixes applied. Never use print().
  </action>
  <verify>
  Run `python -c "from astraea.validation.autofix import AutoFixer, FixAction, FixClassification, IssueClassification"` -- should import without error.
  Run `ruff check src/astraea/validation/autofix.py` -- should pass.
  </verify>
  <done>AutoFixer class exists with classify_issue() covering all rule IDs, 6+ fix functions, FixAction audit model, and FixClassification enum. All auto-fixable categories from the success criteria are covered: CT case, missing DOMAIN/STUDYID, name/label truncation, ASCII.</done>
</task>

<task type="auto">
  <name>Task 2: Unit tests for AutoFixer classification and fix functions</name>
  <files>tests/unit/validation/test_autofix.py</files>
  <action>
Create `tests/unit/validation/test_autofix.py` with comprehensive unit tests:

1. **Classification tests** (test_classify_*):
   - `test_classify_ct_case_mismatch_as_auto_fixable`: Create RuleResult with rule_id="ASTR-T001", variable="SEX", message containing "'male'" (case mismatch). Build a mock CTReference where SEX codelist C66731 has term "MALE". Assert classification == AUTO_FIXABLE.
   - `test_classify_ct_wrong_value_as_needs_human`: Create RuleResult for ASTR-T001 with value "UNKNOWN_VALUE" that doesn't match any codelist term even case-insensitively. Assert NEEDS_HUMAN.
   - `test_classify_domain_missing_as_auto_fixable`: ASTR-T002 with "DOMAIN column is missing" message. Assert AUTO_FIXABLE.
   - `test_classify_required_studyid_as_auto_fixable`: ASTR-P001 with variable="STUDYID". Assert AUTO_FIXABLE.
   - `test_classify_required_other_as_needs_human`: ASTR-P001 with variable="AETERM". Assert NEEDS_HUMAN.
   - `test_classify_name_length_as_auto_fixable`: ASTR-L001. Assert AUTO_FIXABLE.
   - `test_classify_label_length_as_auto_fixable`: ASTR-L002. Assert AUTO_FIXABLE.
   - `test_classify_char_length_as_needs_human`: ASTR-L003. Assert NEEDS_HUMAN.
   - `test_classify_ascii_as_auto_fixable`: ASTR-F002. Assert AUTO_FIXABLE.
   - `test_classify_date_format_as_needs_human`: ASTR-F001. Assert NEEDS_HUMAN.
   - `test_classify_fda_business_as_needs_human`: FDAB057. Assert NEEDS_HUMAN.
   - `test_classify_cross_domain_as_needs_human`: ASTR-C001. Assert NEEDS_HUMAN.

2. **Fix function tests** (test_fix_*):
   - `test_fix_ct_case_normalization`: Create DataFrame with SEX column containing ["male", "FEMALE", "Male"]. Mock CTReference with C66731 terms {"M": ..., "F": ...} -- wait, the actual codelist has submission_values. Actually, look at how CTValueRule works: it checks `v not in cl.terms` where cl.terms is a dict of submission_value -> preferred_term. So for SEX, terms might be {"M": "MALE", "F": "FEMALE"}. For case fix: the invalid values are ones where value.upper() or value matches a key case-insensitively. Build the test accordingly. After fix, verify values corrected. Verify FixAction audit trail has before/after.
   - `test_fix_domain_column_missing`: DataFrame without DOMAIN column. After fix, DOMAIN column exists with correct value.
   - `test_fix_domain_column_wrong_value`: DataFrame with DOMAIN="XX" instead of "AE". After fix, all values = "AE".
   - `test_fix_missing_studyid`: DataFrame without STUDYID. After fix, STUDYID = spec.study_id.
   - `test_fix_variable_name_truncation`: DataFrame with column "LONGVARNAME". After fix, column renamed to "LONGVARN" (8 chars).
   - `test_fix_audit_trail_completeness`: Apply a fix and verify FixAction has all required fields populated (rule_id, domain, variable, fix_type, before_value, after_value, affected_count, timestamp).
   - `test_apply_fixes_returns_copy`: Verify original DataFrame is not modified.
   - `test_apply_fixes_skips_needs_human`: Pass a mix of auto-fixable and needs-human issues. Verify only auto-fixable ones get fix actions.

For mock CTReference: Use the real CTReference if possible (it loads from bundled JSON). If not, create a minimal mock with `lookup_codelist()` returning a codelist object with appropriate terms dict and extensible flag.

For mock SDTMReference: Use the real SDTMReference (loads from bundled JSON).

For DomainMappingSpec in tests: Create minimal spec with `domain="AE"`, `study_id="TEST-001"`, and a few variable_mappings.

Use pytest fixtures for common test setup (AutoFixer instance, sample DataFrames, sample RuleResults).
  </action>
  <verify>
  Run `pytest tests/unit/validation/test_autofix.py -v` -- all tests pass.
  Run `ruff check tests/unit/validation/test_autofix.py` -- clean.
  </verify>
  <done>12+ classification tests and 8+ fix function tests pass. Every auto-fixable category has at least one test. Audit trail completeness verified. DataFrame immutability verified.</done>
</task>

</tasks>

<verification>
- `python -c "from astraea.validation.autofix import AutoFixer, FixAction, FixClassification"` succeeds
- `pytest tests/unit/validation/test_autofix.py -v` -- all tests pass
- `ruff check src/astraea/validation/autofix.py tests/unit/validation/test_autofix.py` -- clean
- AutoFixer.classify_issue() handles all known rule IDs
- Each auto-fixable category from success criteria has a fix function and test
</verification>

<success_criteria>
1. AutoFixer classifies every validation rule_id as auto-fixable or needs-human
2. Fix functions exist for: CT case normalization, missing DOMAIN/STUDYID, name truncation, label truncation, ASCII fixes
3. FixAction audit model captures before/after values for every fix
4. All unit tests pass
5. Code is ruff-clean and type-hinted
</success_criteria>

<output>
After completion, create `.planning/phases/07.1-auto-fix-validation-issues/07.1-01-SUMMARY.md`
</output>
