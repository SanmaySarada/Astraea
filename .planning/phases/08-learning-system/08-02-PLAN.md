---
phase: 08-learning-system
plan: 02
type: execute
wave: 2
depends_on: ["08-01"]
files_modified:
  - src/astraea/learning/retriever.py
  - src/astraea/mapping/engine.py
  - src/astraea/mapping/prompts.py
  - tests/unit/learning/test_retriever.py
  - tests/unit/mapping/test_engine_learning.py
autonomous: true

must_haves:
  truths:
    - "When mapping a domain, relevant past examples are retrieved and injected into the LLM prompt"
    - "When no learning data exists (cold start), the mapping engine works identically to before"
    - "Corrections are prioritized over plain approvals in retrieval (up to 3 corrections, then fill with approvals)"
    - "Retrieved examples are formatted as a readable prompt section, not raw JSON"
  artifacts:
    - path: "src/astraea/learning/retriever.py"
      provides: "LearningRetriever that queries vector store and formats examples for prompt injection"
      exports: ["LearningRetriever"]
    - path: "src/astraea/mapping/engine.py"
      provides: "MappingEngine with optional learning_retriever parameter"
      contains: "learning_retriever"
  key_links:
    - from: "src/astraea/learning/retriever.py"
      to: "src/astraea/learning/vector_store.py"
      via: "queries LearningVectorStore for similar mappings"
      pattern: "LearningVectorStore"
    - from: "src/astraea/mapping/engine.py"
      to: "src/astraea/learning/retriever.py"
      via: "optional dependency injection"
      pattern: "learning_retriever.*LearningRetriever"
---

<objective>
Build the retriever that pulls relevant past examples from the vector store and integrate it into the MappingEngine so that few-shot examples are automatically injected into mapping prompts when learning data is available.

Purpose: This is the core value proposition -- the mapping engine gets better with accumulated corrections by seeing relevant past examples in its prompt context.
Output: LearningRetriever class + MappingEngine integration with backward-compatible optional parameter.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/08-learning-system/08-RESEARCH.md

@src/astraea/mapping/engine.py
@src/astraea/mapping/context.py
@src/astraea/mapping/prompts.py
@src/astraea/learning/models.py
@src/astraea/learning/vector_store.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: LearningRetriever and prompt formatting</name>
  <files>
    src/astraea/learning/retriever.py
    tests/unit/learning/test_retriever.py
  </files>
  <action>
    Create `src/astraea/learning/retriever.py` with class **LearningRetriever**:

    - `__init__(self, vector_store: LearningVectorStore)` -- stores vector_store reference.

    - `get_examples_section(self, *, domain: str, source_profiles: list[DatasetProfile], max_examples: int = 5) -> str | None`:
      1. Build a query text from the domain and source profile variable names/labels (e.g., "SDTM domain AE mapping. Source variables: AETERM (Reported Term), AESTDT (Start Date), ..."). Use first 10 clinical (non-EDC) variables.
      2. Query corrections collection first: `query_similar_corrections(domain, query_text, n_results=3)`. Filter to non-invalidated.
      3. Query approved mappings: `query_similar_mappings(domain, query_text, n_results=max_examples)`.
      4. If both empty, return None (cold start graceful degradation).
      5. Call `format_examples_section(approved_results, correction_results, max_total=max_examples)` and return the formatted string.

    - `format_examples_section(self, approved: list[dict], corrections: list[dict], max_total: int = 5) -> str`:
      Format into a markdown section following RESEARCH.md pattern:
      ```
      ## Relevant Past Mapping Examples

      The following examples are from previously approved mappings for similar variables.
      Use them as reference, but adapt to the current source data.

      ### Correction Example 1
      Variable: AEDECOD
      WRONG approach: Direct carry from AETERM
      CORRECT approach: Derive via MedDRA coding lookup
      Reason: AEDECOD requires dictionary-derived preferred term, not verbatim text

      ### Approved Example 1
      Variable: AETERM (AE)
      Pattern: direct
      Logic: Direct carry from source ae.AETERM
      Source: AETERM
      ```
      Corrections first (up to 3), then fill remaining with approved examples.
      Parse correction metadata for original_logic/corrected_logic from the document text and metadata.
      If the document text for a correction contains "WRONG:" and "CORRECT:" markers, use those. Otherwise, use correction_type and metadata to reconstruct.

    - `build_query_text(self, domain: str, source_profiles: list[DatasetProfile]) -> str`:
      Combine domain name with first 10 variable names and labels from source profiles (filtering out EDC columns via is_edc_column flag). Return natural language description.

    Tests in `tests/unit/learning/test_retriever.py`:
    - Mock LearningVectorStore. Test get_examples_section returns None when both queries return empty (cold start).
    - Test get_examples_section returns formatted string when corrections exist.
    - Test get_examples_section returns formatted string with only approved (no corrections).
    - Test corrections are prioritized (appear before approved examples).
    - Test max_total is respected (3 corrections + 2 approved = 5 total when max_total=5).
    - Test build_query_text includes variable names and labels.
    - Test format_examples_section output format.
  </action>
  <verify>pytest tests/unit/learning/test_retriever.py -v passes all tests</verify>
  <done>LearningRetriever queries vector store, formats examples as readable prompt section, returns None on cold start</done>
</task>

<task type="auto">
  <name>Task 2: Wire LearningRetriever into MappingEngine</name>
  <files>
    src/astraea/mapping/engine.py
    src/astraea/mapping/prompts.py
    tests/unit/mapping/test_engine_learning.py
  </files>
  <action>
    Modify `src/astraea/mapping/engine.py`:
    1. Add optional `learning_retriever: LearningRetriever | None = None` parameter to `__init__()`.
    2. Store as `self._learning = learning_retriever`.
    3. In `map_domain()`, after building the context prompt (Step 2) and before appending user instructions (Step 3), add:
       ```python
       # Step 2.5: Inject learning examples if available
       examples_section = None
       if self._learning is not None:
           examples_section = self._learning.get_examples_section(
               domain=domain,
               source_profiles=source_profiles,
               max_examples=5,
           )
       ```
    4. When constructing `full_prompt`, insert examples_section between prompt and user_instructions if not None:
       ```python
       if examples_section:
           full_prompt = prompt + "\n\n" + examples_section + "\n\n" + user_instructions
       else:
           full_prompt = prompt + "\n" + user_instructions
       ```
    5. Log when examples are injected: `logger.info("Injected {n} learning examples for {domain}", n=..., domain=domain)`

    Modify `src/astraea/mapping/prompts.py`:
    - Add a brief instruction to `MAPPING_SYSTEM_PROMPT` (at the end, before the closing instruction): "If past mapping examples are provided below, use them as reference for similar variables. Adapt the patterns to the current source data -- do not copy them blindly."
    - This instruction should be conditional-friendly: it doesn't hurt when no examples are present.

    IMPORTANT: The import of LearningRetriever in engine.py should use `TYPE_CHECKING` to avoid circular imports and keep chromadb as an optional dependency at import time:
    ```python
    from __future__ import annotations
    from typing import TYPE_CHECKING
    if TYPE_CHECKING:
        from astraea.learning.retriever import LearningRetriever
    ```

    Create `tests/unit/mapping/test_engine_learning.py`:
    - Test MappingEngine.__init__ accepts learning_retriever=None (backward compat).
    - Test MappingEngine.__init__ accepts a mock LearningRetriever.
    - Test that when learning_retriever is None, map_domain works identically to before (mock LLM call, verify prompt does NOT contain "Past Mapping Examples").
    - Test that when learning_retriever returns examples, they appear in the prompt passed to LLM (mock LLM, capture prompt, assert "Past Mapping Examples" in prompt).
    - Test that when learning_retriever returns None (cold start), prompt is unchanged.
  </action>
  <verify>pytest tests/unit/mapping/test_engine_learning.py -v passes all tests && pytest tests/unit/mapping/ -v passes (no regressions)</verify>
  <done>MappingEngine accepts optional LearningRetriever, injects examples into prompts when available, works identically to before when not provided</done>
</task>

</tasks>

<verification>
- `pytest tests/unit/learning/test_retriever.py tests/unit/mapping/test_engine_learning.py -v` -- all tests pass
- `pytest tests/unit/mapping/ -v` -- no regressions in existing mapping tests
- `ruff check src/astraea/learning/retriever.py src/astraea/mapping/engine.py src/astraea/mapping/prompts.py`
- Verify backward compatibility: `python -c "from astraea.mapping.engine import MappingEngine; print('OK')"` works without chromadb installed
</verification>

<success_criteria>
- LearningRetriever retrieves and formats few-shot examples from vector store
- MappingEngine accepts optional learning_retriever parameter (backward compatible)
- When examples available, they appear in the LLM prompt
- When no examples (cold start), behavior is identical to pre-learning-system
- All existing mapping tests still pass
</success_criteria>

<output>
After completion, create `.planning/phases/08-learning-system/08-02-SUMMARY.md`
</output>
