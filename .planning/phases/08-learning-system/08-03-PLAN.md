---
phase: 08-learning-system
plan: 03
type: execute
wave: 2
depends_on: ["08-01"]
files_modified:
  - src/astraea/learning/metrics.py
  - src/astraea/learning/ingestion.py
  - tests/unit/learning/test_metrics.py
  - tests/unit/learning/test_ingestion.py
autonomous: true

must_haves:
  truths:
    - "After a domain review completes, approved mappings and corrections are automatically ingested into the learning stores"
    - "Accuracy metrics are computed per domain per study from review decisions"
    - "Metrics track improvement over time (accuracy_rate, correction_rate per study)"
    - "Ingestion is idempotent -- re-ingesting the same review does not create duplicates"
  artifacts:
    - path: "src/astraea/learning/ingestion.py"
      provides: "Pipeline to extract approved mappings and corrections from DomainReview into learning stores"
      exports: ["ingest_review_results", "ingest_session"]
    - path: "src/astraea/learning/metrics.py"
      provides: "Accuracy computation from review decisions and cross-study comparison"
      exports: ["compute_domain_accuracy", "compute_improvement_report"]
  key_links:
    - from: "src/astraea/learning/ingestion.py"
      to: "src/astraea/review/models.py"
      via: "reads DomainReview, ReviewDecision, HumanCorrection"
      pattern: "from astraea\\.review\\.models import"
    - from: "src/astraea/learning/ingestion.py"
      to: "src/astraea/learning/example_store.py"
      via: "saves MappingExample and CorrectionRecord"
      pattern: "ExampleStore"
    - from: "src/astraea/learning/ingestion.py"
      to: "src/astraea/learning/vector_store.py"
      via: "indexes examples in ChromaDB"
      pattern: "LearningVectorStore"
---

<objective>
Build the ingestion pipeline that extracts learning data from completed reviews and the accuracy metrics tracker that measures improvement over time.

Purpose: Data flows IN to the learning system via ingestion (the other half of the loop -- Plan 02 handles data flowing OUT as few-shot examples). Metrics prove the system actually improves.
Output: ingestion.py (review -> learning stores) and metrics.py (accuracy tracking).
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/08-learning-system/08-RESEARCH.md

@src/astraea/review/models.py
@src/astraea/review/session.py
@src/astraea/learning/models.py
@src/astraea/learning/example_store.py
@src/astraea/learning/vector_store.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Accuracy metrics computation</name>
  <files>
    src/astraea/learning/metrics.py
    tests/unit/learning/test_metrics.py
  </files>
  <action>
    Create `src/astraea/learning/metrics.py`:

    1. **`compute_domain_accuracy(domain_review: DomainReview, study_id: str) -> StudyMetrics`**:
       - Count decisions by status: approved (APPROVED), corrected (CORRECTED with type != REJECT), rejected (CORRECTED with type == REJECT), added (CORRECTED with type == ADD).
       - total_proposed = len(domain_review.original_spec.variable_mappings)
       - accuracy_rate = approved_unchanged / total_proposed (0.0 if total is 0)
       - correction_rate = corrected / total_proposed (0.0 if total is 0)
       - Return StudyMetrics with all fields populated.

    2. **`compute_improvement_report(metrics_list: list[StudyMetrics]) -> dict`**:
       - Takes a list of StudyMetrics (across studies/domains).
       - Groups by domain.
       - For each domain, sorts by completed_at.
       - Computes: first_accuracy, latest_accuracy, improvement (latest - first), trend (list of accuracy_rate values over time).
       - Returns dict: `{"overall_accuracy": float, "by_domain": {domain: {"first": float, "latest": float, "improvement": float, "studies": int, "trend": [float]}}, "total_examples": int, "total_corrections": int}`.
       - overall_accuracy is the mean of all accuracy_rates across all entries.

    3. **`format_improvement_summary(report: dict) -> str`**:
       - Formats the improvement report as a human-readable string for CLI display.
       - Shows per-domain accuracy trends and overall improvement.

    Tests in `tests/unit/learning/test_metrics.py`:
    - Test compute_domain_accuracy with all approved (100% accuracy).
    - Test compute_domain_accuracy with mixed (some corrected, some rejected).
    - Test compute_domain_accuracy with zero mappings (edge case).
    - Test compute_improvement_report with single study (no improvement to measure).
    - Test compute_improvement_report with multiple studies showing improvement.
    - Test compute_improvement_report groups by domain correctly.
    - Test format_improvement_summary produces non-empty string.

    Use mock DomainReview objects with ReviewDecision dicts. Do NOT import the full review pipeline -- construct test data directly from the Pydantic models.
  </action>
  <verify>pytest tests/unit/learning/test_metrics.py -v passes all tests</verify>
  <done>Accuracy metrics computed from review decisions, improvement tracking across studies works</done>
</task>

<task type="auto">
  <name>Task 2: Review-to-learning ingestion pipeline</name>
  <files>
    src/astraea/learning/ingestion.py
    tests/unit/learning/test_ingestion.py
  </files>
  <action>
    Create `src/astraea/learning/ingestion.py`:

    1. **`ingest_domain_review(domain_review: DomainReview, study_id: str, example_store: ExampleStore, vector_store: LearningVectorStore) -> int`**:
       - Extract the reviewed spec (use reviewed_spec if available, else original_spec).
       - For each VariableMapping in the spec:
         - Create MappingExample with:
           - study_id, domain from spec
           - sdtm_variable, mapping_pattern, mapping_logic, source_variable, source_dataset from mapping
           - source_label from mapping (if available via the source_sas_label field or similar)
           - confidence from mapping
           - was_corrected = True if this variable has a CORRECTED decision
           - final_mapping_json = mapping.model_dump_json()
         - Save to example_store
         - Add to vector_store
       - For each correction in domain_review.corrections:
         - Create CorrectionRecord with:
           - study_id, session_id from domain_review context
           - domain, sdtm_variable from correction
           - correction_type from correction.correction_type
           - original_pattern = correction.original_mapping.mapping_pattern
           - corrected_pattern = correction.corrected_mapping.mapping_pattern if corrected_mapping exists
           - original_logic = correction.original_mapping.mapping_logic
           - corrected_logic = correction.corrected_mapping.mapping_logic if exists
           - reason = correction.reason
         - Save to example_store
         - Add to vector_store
       - Return count of examples + corrections ingested.

    2. **`ingest_session(session: ReviewSession, example_store: ExampleStore, vector_store: LearningVectorStore) -> dict`**:
       - Iterate over all domain_reviews in the session.
       - Call ingest_domain_review for each completed domain review.
       - Compute and save StudyMetrics for each domain via compute_domain_accuracy.
       - Return summary dict: {"total_examples": int, "total_corrections": int, "domains_ingested": list[str]}.

    IMPORTANT: Use INSERT OR IGNORE for examples (based on a deterministic ID derived from study_id + domain + sdtm_variable) to make ingestion idempotent. If re-ingesting, skip already-existing examples rather than duplicating.

    Generate deterministic example_id as: f"{study_id}_{domain}_{sdtm_variable}" for MappingExample (overriding the UUID default). For CorrectionRecord, use: f"{session_id}_{domain}_{sdtm_variable}_{correction_type}".

    Tests in `tests/unit/learning/test_ingestion.py`:
    - Test ingest_domain_review with 3 approved + 1 corrected mapping -> 4 examples + 1 correction stored.
    - Test ingest_domain_review correctly sets was_corrected flag.
    - Test idempotency: call ingest_domain_review twice, verify no duplicates.
    - Test ingest_session processes multiple domains.
    - Test ingest_session computes and stores metrics.
    - Test with domain review that has no corrections (all approved).

    Use real SQLite (tmp_path) and in-memory ChromaDB for tests. Create mock DomainReview objects with the required structure.
  </action>
  <verify>pytest tests/unit/learning/test_ingestion.py -v passes all tests</verify>
  <done>Ingestion pipeline extracts learning data from reviews, stores in both SQLite and ChromaDB, idempotent on re-ingestion, metrics computed and saved</done>
</task>

</tasks>

<verification>
- `pytest tests/unit/learning/ -v` -- all learning tests pass (models, stores, metrics, ingestion)
- `ruff check src/astraea/learning/`
- No changes to existing review/ modules (ingestion reads from review models but doesn't modify them)
</verification>

<success_criteria>
- Ingestion extracts all approved mappings and corrections from DomainReview into both stores
- StudyMetrics computed correctly from review decisions
- Improvement report shows accuracy trends across studies
- Idempotent ingestion (no duplicates on re-run)
- All tests pass, ruff clean
</success_criteria>

<output>
After completion, create `.planning/phases/08-learning-system/08-03-SUMMARY.md`
</output>
